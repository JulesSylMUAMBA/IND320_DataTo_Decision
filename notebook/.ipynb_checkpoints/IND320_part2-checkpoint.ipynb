{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250abd8c-353f-4c66-b0c6-4a8bec30bdc8",
   "metadata": {},
   "source": [
    "# IND320 – Project Part 2 : Elhub Data Analysis\n",
    "## *Welcome to my Project work, part 2 - Data Sources*\n",
    "### Author : Jules Sylvain MUAMBA MVELE\n",
    "### Description :\n",
    "This notebook demonstrates the Elhub data analysis workflow for 2021.\n",
    "It includes :\n",
    "- Data ingestion from Elhub (CSV or API)\n",
    "- Storage in Cassandra and MongoDB\n",
    "- Visualizations (pie chart and line chart)\n",
    "- Documentation of AI assistance and work log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed507f30-c75b-457b-8e1d-d33395412665",
   "metadata": {},
   "source": [
    "## Part 1 : Diagnostic and Troubleshooting – Spark & Cassandra Configuration on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedbde8-9eb2-4e23-9e4c-87fc70c3d840",
   "metadata": {},
   "source": [
    "During the setup of the Spark + Cassandra environment, several technical issues were encountered.\n",
    "This section documents the problems, the attempted fixes, and the final working configuration, it's not working anyway, i'm open to any help.\n",
    "I tried a lot of things for days, hope it's clear enough to understand (even if it's not clear for me anymore haha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887e792-7072-460b-a19d-03c281a4b986",
   "metadata": {},
   "source": [
    "### Initial Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a91852-b149-4a9b-af6e-27506b2e9b8b",
   "metadata": {},
   "source": [
    "The goal was to run a Jupyter (Python) notebook capable of:\n",
    "\n",
    "Starting a local Spark 3.5.1 session\n",
    "\n",
    "Connecting to Cassandra 5.0 running in a Docker container\n",
    "\n",
    "Using the connector spark-cassandra-connector_2.12-3.5.1.jar\n",
    "\n",
    "System setup: Windows 11, Anaconda, environment IND320_env, Java JDK 17 (Temurin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a676156-4442-438e-9cd8-0e1fed33bf90",
   "metadata": {},
   "source": [
    "### Problems Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e5a61-1586-4702-9afa-075918d19d50",
   "metadata": {},
   "source": [
    "#### **1. Error ModuleNotFoundError: No module named 'pyspark'**\n",
    "\n",
    "- Cause: The PySpark package was not installed in the conda environment.\n",
    "- Solution: \"pip install pyspark\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee315c60-d29d-4318-b90e-f6d9b0d3af7b",
   "metadata": {},
   "source": [
    "#### **2. Error PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number**\n",
    "\n",
    "- Error appeared when launching Spark: \"from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\"\n",
    "- Analysis:\n",
    "\n",
    "Communication issue between Python and the JVM\n",
    "\n",
    "Missing environment variables: JAVA_HOME, HADOOP_HOME, SPARK_HOME\n",
    "\n",
    "Missing winutils on Windows\n",
    "- Complete Solution:\n",
    "\n",
    "Manually install Hadoop winutils (v3.3.1) from the kontext-tech/winutils repository\n",
    "\n",
    "Download and extract Spark 3.5.1 prebuilt for Hadoop 3 into C:\\spark\\spark-3.5.1-bin-hadoop3\n",
    "\n",
    "Set the environment variables: \n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "```\n",
    "\n",
    "- Test with: \"%SPARK_HOME%\\bin\\spark-submit.cmd --version\"\n",
    "- Result : \"Spark 3.5.1 runs successfully with Scala 2.12.18 and Java 17.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615b0c9-865e-4653-a5bd-bf5a0987c78a",
   "metadata": {},
   "source": [
    "#### **3. Error TypeError: 'JavaPackage' object is not callable**\n",
    "\n",
    "- Cause: Spark environment not properly initialized or incorrect path.\n",
    "- Solution: Explicitly define SPARK_HOME and use findspark for diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeb446-46dc-4f2c-a6a7-eab62da0ed87",
   "metadata": {},
   "source": [
    "#### **4. Hadoop-related error: /tmp not accessible**\n",
    "\n",
    "- Cause: Spark on Windows requires a valid /tmp directory with proper permissions.\n",
    "- Solution:\n",
    "```\n",
    "import os, subprocess\n",
    "os.makedirs(r\"C:\\tmp\", exist_ok=True)\n",
    "subprocess.run(r'C:\\Hadoop\\hadoop-3.3.1\\bin\\winutils.exe chmod 777 /tmp', shell=True)\n",
    "\n",
    "```\n",
    "\n",
    "- Spark starts without warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b77cc9-b9b7-41ff-8d17-60959760dc61",
   "metadata": {},
   "source": [
    "#### **5. Spark–Cassandra Connector Compatibility Error**\n",
    "\n",
    "- Cause: The connector was not automatically downloaded.\n",
    "- Solution: Manually download et and configure explicitly::\n",
    "```\n",
    "spark-cassandra-connector_2.12-3.5.1.jar\n",
    ".config('spark.jars', r'C:\\...\\spark-cassandra-connector_2.12-3.5.1.jar')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeb8ea-c126-4570-a7ba-2e0ebc679546",
   "metadata": {},
   "source": [
    "#### **Lessons Learned**\n",
    "\n",
    "On Windows, Spark requires Java + Hadoop (winutils) + IPv4 configuration.\n",
    "\n",
    "The JAVA_GATEWAY_EXITED error is almost always caused by incorrect environment settings.\n",
    "\n",
    "Testing Spark via spark-submit.cmd --version before using PySpark in Jupyter helps isolate configuration issues.\n",
    "\n",
    "Version compatibility between Spark (3.5.1), Scala (2.12), and the Cassandra connector is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b245ed-665b-41fe-a51a-9f2d54354655",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ad3f0-4e19-4e6e-8296-64cf24746d95",
   "metadata": {},
   "source": [
    "### Final Config tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46f724d-b477-4aa5-b3da-0378625d2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark environment + Cassandra ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Spark and Cassandra paths ---\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "jar_path = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector_2.12-3.5.1.jar\"\n",
    "\n",
    "# --- Force Spark to include the JAR in the classpath ---\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f'--jars \"{jar_path}\" pyspark-shell'\n",
    "\n",
    "# --- Java and Hadoop ---\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"PATH\"] = (\n",
    "    os.environ[\"JAVA_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"SPARK_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"PATH\"]\n",
    ")\n",
    "\n",
    "# --- IPv4 / Python configuration ---\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "print(\"Spark environment + Cassandra ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1736f89a-993b-4b59-98c1-9f7811bec058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winutils chmod done\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "os.makedirs(r\"C:\\tmp\", exist_ok=True)\n",
    "subprocess.run(r'C:\\Hadoop\\hadoop-3.3.1\\bin\\winutils.exe chmod 777 /tmp', shell=True, check=False)\n",
    "print(\"winutils chmod done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654914-cb24-488c-8c82-aac0f452b69b",
   "metadata": {},
   "source": [
    "import os, findspark\n",
    "# s'assure que SPARK_HOME est bien connu du notebook\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "findspark.init(os.environ[\"SPARK_HOME\"])  # <-- clé\n",
    "print(\"findspark OK ->\", findspark.find())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9971b-b01e-4020-a059-27dd583225fb",
   "metadata": {},
   "source": [
    "import os, sys, findspark\n",
    "\n",
    "# --- Local paths ---\n",
    "SPARK_HOME  = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "JAVA_HOME   = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "HADOOP_HOME = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "\n",
    "# Cassandra JARs (assembly + driver)\n",
    "jar_main   = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector-assembly_2.12-3.5.1.jar\"\n",
    "jar_driver = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector-driver_2.12-3.5.1.jar\"\n",
    "all_jars   = f\"{jar_main},{jar_driver}\"\n",
    "\n",
    "# --- Environment variables for Spark / Java / Hadoop / Python ---\n",
    "os.environ[\"SPARK_HOME\"]  = SPARK_HOME\n",
    "os.environ[\"JAVA_HOME\"]   = JAVA_HOME\n",
    "os.environ[\"HADOOP_HOME\"] = HADOOP_HOME\n",
    "os.environ[\"PATH\"] = (\n",
    "    SPARK_HOME + r\"\\bin;\"\n",
    "    + HADOOP_HOME + r\"\\bin;\"\n",
    "    + JAVA_HOME + r\"\\bin;\"\n",
    "    + os.environ[\"PATH\"]\n",
    ")\n",
    "\n",
    "# Force PySpark to use the current Python interpreter (important on Windows)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# Network configuration (Windows)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "# Load JARs on both driver and executors (also through submit args)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f'--jars \"{all_jars}\" pyspark-shell'\n",
    "\n",
    "# Initialize Spark with findspark\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Create Spark Session ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[1]\")  # More stable on Windows\n",
    "    .appName(\"SparkCassandraApp\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")  # Docker Desktop → Cassandra\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    .config(\"spark.python.worker.reuse\", \"false\")\n",
    "    .config(\"spark.driver.extraClassPath\", all_jars)\n",
    "    .config(\"spark.executor.extraClassPath\", all_jars)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"✅ Spark initialized — version:\", spark.version)\n",
    "print(\"✅ Spark JARs:\", spark.sparkContext.getConf().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21a606-736a-478d-92fc-10269da49279",
   "metadata": {},
   "source": [
    "print(\"JARs used :\", spark.sparkContext._conf.get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52c4c29-899c-41a9-b4c9-6dc333199e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostbyname(\"localhost\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e02a978-2570-434e-bdb9-8ea80c9eab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra is on 9042\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "s = socket.socket()\n",
    "s.settimeout(2)\n",
    "try:\n",
    "    s.connect((\"localhost\", 9042))\n",
    "    print(\"Cassandra is on 9042\")\n",
    "except Exception as e:\n",
    "    print(\" Cassandra not avaible\", e)\n",
    "s.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa830a-ebf1-4aa0-a024-453ba1234eb8",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"PATH\"] = os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d22f9-a15a-413a-bb35-b9c332938e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b41d6ad3-1762-4ada-85e2-7cc680877c1f",
   "metadata": {},
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PYTHON_EXE = r\"C:\\Users\\muamb\\anaconda3\\envs\\IND320_env\\python.exe\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYTHON_EXE\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYTHON_EXE\n",
    "for bad in (\"PYTHONHOME\", \"PYTHONPATH\"):\n",
    "    os.environ.pop(bad, None)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"worker-smoke-test\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"✅ Spark:\", spark.version)\n",
    "print(\"✅ Python:\", sys.executable)\n",
    "print(\"➡️ Test workers =\", spark.sparkContext.parallelize(range(10)).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895a0e2-61b9-4cba-a187-c49e85cc3895",
   "metadata": {},
   "source": [
    "### **Main Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995ab11-103b-4657-ae8e-8363187a08a4",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "Cell In[17], line 20\n",
    "     18 print(\"✅ Spark:\", spark.version)\n",
    "     19 print(\"✅ Python:\", sys.executable)\n",
    "---> 20 print(\"➡️ Test workers =\", spark.sparkContext.parallelize(range(10)).count())\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2316, in RDD.count(self)\n",
    "   2295 def count(self) -> int:\n",
    "   2296     \"\"\"\n",
    "   2297     Return the number of elements in this RDD.\n",
    "   2298 \n",
    "   (...)   2314     3\n",
    "   2315     \"\"\"\n",
    "-> 2316     return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2291, in RDD.sum(self)\n",
    "   2270 def sum(self: \"RDD[NumberOrArray]\") -> \"NumberOrArray\":\n",
    "   2271     \"\"\"\n",
    "   2272     Add up the elements in this RDD.\n",
    "   2273 \n",
    "   (...)   2289     6.0\n",
    "   2290     \"\"\"\n",
    "-> 2291     return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n",
    "   2292         0, operator.add\n",
    "   2293     )\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2044, in RDD.fold(self, zeroValue, op)\n",
    "   2039     yield acc\n",
    "   2041 # collecting result of mapPartitions here ensures that the copy of\n",
    "   2042 # zeroValue provided to each partition is unique from the one provided\n",
    "   2043 # to the final reduce call\n",
    "-> 2044 vals = self.mapPartitions(func).collect()\n",
    "   2045 return reduce(op, vals, zeroValue)\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:1833, in RDD.collect(self)\n",
    "   1831 with SCCallSiteSync(self.context):\n",
    "   1832     assert self.ctx._jvm is not None\n",
    "-> 1833     sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
    "   1834 return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322, in JavaMember.__call__(self, *args)\n",
    "   1316 command = proto.CALL_COMMAND_NAME +\\\n",
    "   1317     self.command_header +\\\n",
    "   1318     args_command +\\\n",
    "   1319     proto.END_COMMAND_PART\n",
    "   1321 answer = self.gateway_client.send_command(command)\n",
    "-> 1322 return_value = get_return_value(\n",
    "   1323     answer, self.gateway_client, self.target_id, self.name)\n",
    "   1325 for temp_arg in temp_args:\n",
    "   1326     if hasattr(temp_arg, \"_detach\"):\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179, in capture_sql_exception.<locals>.deco(*a, **kw)\n",
    "    177 def deco(*a: Any, **kw: Any) -> Any:\n",
    "    178     try:\n",
    "--> 179         return f(*a, **kw)\n",
    "    180     except Py4JJavaError as e:\n",
    "    181         converted = convert_exception(e.java_exception)\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n",
    "    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n",
    "    325 if answer[1] == REFERENCE_TYPE:\n",
    "--> 326     raise Py4JJavaError(\n",
    "    327         \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "    328         format(target_id, \".\", name), value)\n",
    "    329 else:\n",
    "    330     raise Py4JError(\n",
    "    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\n",
    "    332         format(target_id, \".\", name, value))\n",
    "\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 1.0 failed 1 times, most recent failure: Lost task 9.0 in stage 1.0 (TID 21) (kubernetes.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
    "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
    "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
    "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
    "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
    "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
    "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
    "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
    "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
    "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
    "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
    "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
    "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
    "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
    "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
    "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
    "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
    "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
    "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
    "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
    "Caused by: java.io.EOFException\n",
    "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6009ecd0-c05f-40c2-81d2-c50572c8603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\muamb\\anaconda3\\envs\\IND320_py311\\python.exe\n",
      "Version: 3.11.14\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Version:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df852b08-83a4-48d7-9d53-68befc976bc1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9e5f9-37dc-4c94-8f14-0a9591983819",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eca195-6c9a-4370-b4e0-79e4fe269bca",
   "metadata": {},
   "source": [
    "## Part 2 : Work without Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57423b3a-041e-4d6a-83f0-2fd60098783c",
   "metadata": {},
   "source": [
    "### Install all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6e5280-1ed4-4ae8-80d3-6c05ca7e85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: cassandra-driver in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (3.29.3)\n",
      "Requirement already satisfied: pymongo in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (4.15.3)\n",
      "Requirement already satisfied: requests in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: plotly in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: geomet>=1.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from cassandra-driver) (1.1.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pymongo) (2.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from plotly) (2.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from geomet>=1.1->cassandra-driver) (8.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas cassandra-driver pymongo requests matplotlib plotly tqdm python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79bcb1-6e25-42b7-8742-360e0c4484b1",
   "metadata": {},
   "source": [
    "### Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61bc199-9fc0-4e93-91ad-f21a2b82a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79be934-aacf-4799-8f2f-2912c85c55d0",
   "metadata": {},
   "source": [
    "### Test of connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a90ff18-bee5-4357-a20c-f00b2105f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider  # only if your Cassandra requires authentication\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "# --- Connection settings ---\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "\n",
    "\n",
    "CASSANDRA_USER = None  \n",
    "CASSANDRA_PASS = None   \n",
    "\n",
    "KEYSPACE = \"ind320\"\n",
    "TABLE    = \"elhub_data_test\"  # just a temporary test table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c87ef67-cefe-4cdc-ab25-e3e5a56d6ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Cassandra\n"
     ]
    }
   ],
   "source": [
    "# --- Connect to Cassandra cluster ---\n",
    "try:\n",
    "    if CASSANDRA_USER and CASSANDRA_PASS:\n",
    "        auth_provider = PlainTextAuthProvider(\n",
    "            username=CASSANDRA_USER, password=CASSANDRA_PASS\n",
    "        )\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT, auth_provider=auth_provider)\n",
    "    else:\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "\n",
    "    session = cluster.connect()\n",
    "    print(\"✅ Connected to Cassandra\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"❌ Unable to connect to Cassandra: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc8d91dc-7aad-4dce-8bae-ff170400821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keyspace `ind320` and table `elhub_data_test` are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Create keyspace if it does not exist ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "WITH replication = {{'class': 'SimpleStrategy', 'replication_factor': 1}};\n",
    "\"\"\")\n",
    "\n",
    "# --- Switch to the new keyspace ---\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# --- Create a simple test table ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    id int PRIMARY KEY,\n",
    "    txt text\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✅ Keyspace `{KEYSPACE}` and table `{TABLE}` are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572db58a-5a1b-49ca-96b9-eb1f126064fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Rows in the test table (sorted client-side):\n",
      "Row(id=1, txt='hello')\n",
      "Row(id=2, txt='world')\n",
      "Row(id=3, txt='ind320')\n"
     ]
    }
   ],
   "source": [
    "# Sanity check \n",
    "# --- Read without ORDER BY and sort in Python ---\n",
    "rows = session.execute(f\"SELECT id, txt FROM {TABLE};\")\n",
    "rows_sorted = sorted(rows, key=lambda r: r.id)  # sort locally in Python\n",
    "\n",
    "print(\"📄 Rows in the test table (sorted client-side):\")\n",
    "for r in rows_sorted:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc845da5-db75-4c50-81d0-5a5ba44b1086",
   "metadata": {},
   "source": [
    "## Load & clean (CSV → DataFrame pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3c88b-12da-4d4c-aefc-e71489073991",
   "metadata": {},
   "source": [
    "### Load the CSV, inspect it, keep only the useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad10163b-6469-4e70-b3c3-4724cbfacff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded CSV with shape: (661344, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>START_TIME</th>\n",
       "      <th>PRICE_AREA</th>\n",
       "      <th>PRODUCTION_GROUP</th>\n",
       "      <th>QUANTITY_KWH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-24T00:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1518343.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-24T01:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1508836.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-24T02:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1495758.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-24T03:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1491274.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-24T04:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1496936.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      START_TIME PRICE_AREA PRODUCTION_GROUP  QUANTITY_KWH\n",
       "0  2022-10-24T00:00:00.000+02:00        NO1            hydro   1518343.114\n",
       "1  2022-10-24T01:00:00.000+02:00        NO1            hydro   1508836.767\n",
       "2  2022-10-24T02:00:00.000+02:00        NO1            hydro   1495758.356\n",
       "3  2022-10-24T03:00:00.000+02:00        NO1            hydro   1491274.714\n",
       "4  2022-10-24T04:00:00.000+02:00        NO1            hydro   1496936.723"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- File path (adapt if you move the CSV) ---\n",
    "CSV_PATH = Path(r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\IND320_DataTo_Decision\\data\\elhub_data.csv\")\n",
    "\n",
    "# --- Columns we care about for the assignment ---\n",
    "USECOLS = [\"START_TIME\", \"PRICE_AREA\", \"PRODUCTION_GROUP\", \"QUANTITY_KWH\"]\n",
    "\n",
    "# --- Read CSV efficiently: only needed columns, keep strings initially ---\n",
    "df_raw = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    usecols=USECOLS,\n",
    "    dtype={\n",
    "        \"START_TIME\": \"string\",\n",
    "        \"PRICE_AREA\": \"string\",\n",
    "        \"PRODUCTION_GROUP\": \"string\",\n",
    "        \"QUANTITY_KWH\": \"float64\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ Loaded CSV with shape:\", df_raw.shape)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07567702-37f6-43bd-b079-e7b14d1e91a5",
   "metadata": {},
   "source": [
    "### Minimal cleaning + time parsing (handle DST → store in UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3644e81b-dd13-4e32-9575-4690ff282322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with missing critical fields.\n",
      "Removed 0 duplicated hourly rows on key ['price_area', 'production_group', 'start_time'].\n",
      "✅ Clean DataFrame shape: (661344, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>quantity_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-23 22:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1518343.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-23 23:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1508836.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-24 00:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1495758.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-24 01:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1491274.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-24 02:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1496936.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start_time price_area production_group  quantity_kwh\n",
       "0 2022-10-23 22:00:00+00:00        NO1            hydro   1518343.114\n",
       "1 2022-10-23 23:00:00+00:00        NO1            hydro   1508836.767\n",
       "2 2022-10-24 00:00:00+00:00        NO1            hydro   1495758.356\n",
       "3 2022-10-24 01:00:00+00:00        NO1            hydro   1491274.714\n",
       "4 2022-10-24 02:00:00+00:00        NO1            hydro   1496936.723"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Basic column rename to a consistent snake_case schema ---\n",
    "df = df_raw.rename(columns={\n",
    "    \"START_TIME\": \"start_time\",\n",
    "    \"PRICE_AREA\": \"price_area\",\n",
    "    \"PRODUCTION_GROUP\": \"production_group\",\n",
    "    \"QUANTITY_KWH\": \"quantity_kwh\"\n",
    "}).copy()\n",
    "\n",
    "# --- Strip whitespace just in case ---\n",
    "df[\"price_area\"] = df[\"price_area\"].str.strip()\n",
    "df[\"production_group\"] = df[\"production_group\"].str.strip()\n",
    "\n",
    "# --- Parse ISO-8601 timestamps (they contain timezone offsets) to UTC ---\n",
    "# Note: errors='coerce' will produce NaT for bad rows; we'll drop those later.\n",
    "df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# --- Basic integrity checks ---\n",
    "n_before = len(df)\n",
    "df = df.dropna(subset=[\"start_time\", \"price_area\", \"production_group\", \"quantity_kwh\"])\n",
    "n_after = len(df)\n",
    "print(f\"Dropped {n_before - n_after} rows with missing critical fields.\")\n",
    "\n",
    "# --- Ensure proper dtypes ---\n",
    "df[\"price_area\"] = df[\"price_area\"].astype(\"string\")\n",
    "df[\"production_group\"] = df[\"production_group\"].astype(\"string\")\n",
    "df[\"quantity_kwh\"] = df[\"quantity_kwh\"].astype(\"float64\")\n",
    "\n",
    "# --- Optional: remove obvious duplicates if any (same key/hour) ---\n",
    "dedup_cols = [\"price_area\", \"production_group\", \"start_time\"]\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates(subset=dedup_cols, keep=\"last\")\n",
    "print(f\"Removed {n_before - len(df)} duplicated hourly rows on key {dedup_cols}.\")\n",
    "\n",
    "print(\"✅ Clean DataFrame shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59f587b0-a4f4-427a-a369-920f6328f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price areas: ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
      "Production groups: ['*', 'hydro', 'other', 'solar', 'thermal', 'wind']\n",
      "Date range (UTC): 2022-10-23 22:00:00+00:00 → 2025-10-23 21:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>n_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>other</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>thermal</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>wind</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NO2</td>\n",
       "      <td>hydro</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NO2</td>\n",
       "      <td>other</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NO2</td>\n",
       "      <td>thermal</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NO2</td>\n",
       "      <td>solar</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NO2</td>\n",
       "      <td>wind</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price_area production_group  n_rows\n",
       "0         NO1            hydro   26304\n",
       "1         NO1            other   26304\n",
       "2         NO1            solar   26304\n",
       "3         NO1          thermal   26304\n",
       "4         NO1             wind   26304\n",
       "6         NO2            hydro   26304\n",
       "7         NO2            other   26304\n",
       "9         NO2          thermal   26304\n",
       "8         NO2            solar   26304\n",
       "10        NO2             wind   26304"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Quick sanity stats for documentation/log ---\n",
    "print(\"Price areas:\", sorted(df[\"price_area\"].dropna().unique().tolist()))\n",
    "print(\"Production groups:\", sorted(df[\"production_group\"].dropna().unique().tolist()))\n",
    "print(\"Date range (UTC):\", df[\"start_time\"].min(), \"→\", df[\"start_time\"].max())\n",
    "\n",
    "# --- Should be hourly frequency per area/group in 2021; quick check of counts ---\n",
    "counts = df.groupby([\"price_area\", \"production_group\"]).size().reset_index(name=\"n_rows\")\n",
    "counts.sort_values(\"n_rows\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d8c3e-1447-489c-aa10-2b7f41550e3e",
   "metadata": {},
   "source": [
    "### Create Cassandra table (final schema) + insert batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f477860-2f27-4425-a4fb-d8e98c450604",
   "metadata": {},
   "source": [
    "#### Creating the final table (key: (price_area, production_group), clustering: start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "045f4ccf-b3a5-4fac-b108-a5fb7ea114e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table `ind320.elhub_production_mba_hour` ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Create final table for Elhub hourly production ---\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "KEYSPACE = \"ind320\"\n",
    "TABLE = \"elhub_production_mba_hour\"\n",
    "\n",
    "cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# Note: clustering order on start_time for efficient time-range scans within a partition\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    price_area text,\n",
    "    production_group text,\n",
    "    start_time timestamp,\n",
    "    quantity_kwh double,\n",
    "    PRIMARY KEY ((price_area, production_group), start_time)\n",
    ") WITH CLUSTERING ORDER BY (start_time ASC);\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Table `{KEYSPACE}.{TABLE}` ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a387c56-26e3-459e-9455-85e1a18d4f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No rows found for year 2021. Proceeding with ALL rows: 661344 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Optional year filter: keep only a given year if present ---\n",
    "TARGET_YEAR = 2021  # change to 2022/2023/etc. or set to None for \"no filter\"\n",
    "\n",
    "df_to_load = df.copy()\n",
    "if TARGET_YEAR is not None:\n",
    "    mask = df_to_load[\"start_time\"].dt.year == TARGET_YEAR\n",
    "    if mask.any():\n",
    "        df_to_load = df_to_load[mask].copy()\n",
    "        print(f\"ℹ️ Using only rows for year {TARGET_YEAR}: {len(df_to_load)} rows.\")\n",
    "    else:\n",
    "        print(f\"⚠️ No rows found for year {TARGET_YEAR}. Proceeding with ALL rows: {len(df_to_load)} rows.\")\n",
    "else:\n",
    "    print(f\"ℹ️ No year filter. Proceeding with ALL rows: {len(df_to_load)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e285d-0256-4586-a77a-a002b94ccda2",
   "metadata": {},
   "source": [
    "#### Remarks : I don't have any rows for 2021, but i'm sure i downloaded the good dataset, i'll try to find a solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0d2a1d7-fdb8-4c09-ae8b-ab9d66f3125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Inserting 661344 rows into `ind320.elhub_production_mba_hour` (concurrency=64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████| 34/34 [04:03<00:00,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Insert complete. Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Fast concurrent insert using PreparedStatement + execute_concurrent_with_args ---\n",
    "from cassandra.query import PreparedStatement\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Prepare the insert statement once (faster and safer)\n",
    "insert_ps: PreparedStatement = session.prepare(f\"\"\"\n",
    "    INSERT INTO {TABLE} (price_area, production_group, start_time, quantity_kwh)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "# Convert DataFrame rows to tuples expected by the driver\n",
    "params = [\n",
    "    (\n",
    "        str(row[\"price_area\"]),\n",
    "        str(row[\"production_group\"]),\n",
    "        row[\"start_time\"].to_pydatetime(),   # ensure native datetime (UTC)\n",
    "        float(row[\"quantity_kwh\"])\n",
    "    )\n",
    "    for _, row in df_to_load.iterrows()\n",
    "]\n",
    "\n",
    "# Tune concurrency for your machine; start modestly on Windows\n",
    "CONCURRENCY = 64  # you can try 128/256 if it runs smoothly\n",
    "BATCH_SIZE = 20_000  # chunk to avoid huge single job; adjust as needed\n",
    "\n",
    "total = len(params)\n",
    "print(f\" Inserting {total} rows into `{KEYSPACE}.{TABLE}` (concurrency={CONCURRENCY})\")\n",
    "\n",
    "errors_total = 0\n",
    "for i in tqdm(range(0, total, BATCH_SIZE), desc=\"Batches\"):\n",
    "    chunk = params[i:i+BATCH_SIZE]\n",
    "    results = execute_concurrent_with_args(\n",
    "        session,\n",
    "        insert_ps,\n",
    "        chunk,\n",
    "        concurrency=CONCURRENCY,\n",
    "        raise_on_first_error=False\n",
    "    )\n",
    "    # Count failed writes in this chunk\n",
    "    chunk_errors = sum(0 if ok else 1 for ok, _ in results)\n",
    "    errors_total += chunk_errors\n",
    "\n",
    "print(f\" Insert complete. Errors: {errors_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919ab06-b9f5-4498-9ac0-d581b931024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick counts per area/group (rough sanity check) ---\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT price_area, production_group, count(*) as n\n",
    "    FROM {TABLE}\n",
    "    GROUP BY price_area, production_group\n",
    "    ALLOW FILTERING;\n",
    "\"\"\")\n",
    "# Note: ALLOW FILTERING is okay here for small ad-hoc checks; avoid in prod.\n",
    "\n",
    "print(\"🔎 Counts by (price_area, production_group):\")\n",
    "for r in rows:\n",
    "    print(r)\n",
    "\n",
    "# --- Sample read: pick one area/group and a small time window ---\n",
    "sample_area = \"NO1\"\n",
    "sample_group = \"hydro\"\n",
    "\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT price_area, production_group, start_time, quantity_kwh\n",
    "    FROM {TABLE}\n",
    "    WHERE price_area = %s AND production_group = %s\n",
    "    LIMIT 5;\n",
    "\"\"\", (sample_area, sample_group))\n",
    "\n",
    "print(\"\\n🔎 Sample rows:\")\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
