{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250abd8c-353f-4c66-b0c6-4a8bec30bdc8",
   "metadata": {},
   "source": [
    "# IND320 ‚Äì Project Part 2 : Elhub Data Analysis\n",
    "## *Welcome to my Project work, part 2 - Data Sources*\n",
    "### Author : Jules Sylvain MUAMBA MVELE\n",
    "### Description :\n",
    "This notebook demonstrates the Elhub data analysis workflow for 2021.\n",
    "It includes :\n",
    "- Data ingestion from Elhub (CSV or API)\n",
    "- Storage in Cassandra and MongoDB\n",
    "- Visualizations (pie chart and line chart)\n",
    "- Documentation of AI assistance and work log\n",
    "\n",
    "**!! I created a nez branch named part2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422c6d3-f042-4ace-ad63-250d2a445704",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "- **GitHub Repository:** [https://github.com/JulesSylMUAMBA/IND320_DataTo_Decision/tree/part2](https://github.com/JulesSylMUAMBA/IND320_DataTo_Decision/tree/part2)\n",
    "- **Streamlit App:** https://ind320datatodecision-m8z5c8kxibzrnoy2parjfq.streamlit.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9307a-fed1-4eef-ab98-11ecd67927ef",
   "metadata": {},
   "source": [
    "## AI usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a46c90-5993-46d3-a2ee-0fb0b03fb593",
   "metadata": {},
   "source": [
    "During this project, I used AI tools (mainly ChatGPT) as a programming assistant and technical explainer.  \n",
    "The main tasks where AI was used include:\n",
    "- Debugging issues with Python environments (especially Cassandra and package dependencies).\n",
    "- Translating comments and explanations from French to English.\n",
    "- Writing clearly commented code that can be understood by others.\n",
    "- Structuring the project pipeline (Cassandra ‚Üí MongoDB ‚Üí Streamlit).\n",
    "- Clarifying the course requirements and how to implement them in a practical way.\n",
    "\n",
    "The AI was **not used to automatically generate the final results**, but as a guide to understand, fix, and organize the code.  \n",
    "All code was tested and adapted manually.  \n",
    "This use of AI helped improve productivity and understanding without replacing independent work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed507f30-c75b-457b-8e1d-d33395412665",
   "metadata": {},
   "source": [
    "## Part 1 : Diagnostic and Troubleshooting ‚Äì Spark & Cassandra Configuration on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedbde8-9eb2-4e23-9e4c-87fc70c3d840",
   "metadata": {},
   "source": [
    "During the setup of the Spark + Cassandra environment, several technical issues were encountered.\n",
    "This section documents the problems, the attempted fixes, and the final working configuration, it's not working anyway, i'm open to any help.\n",
    "I tried a lot of things for days, hope it's clear enough to understand (even if it's not clear for me anymore haha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887e792-7072-460b-a19d-03c281a4b986",
   "metadata": {},
   "source": [
    "### Initial Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a91852-b149-4a9b-af6e-27506b2e9b8b",
   "metadata": {},
   "source": [
    "The goal was to run a Jupyter (Python) notebook capable of:\n",
    "\n",
    "Starting a local Spark 3.5.1 session\n",
    "\n",
    "Connecting to Cassandra 5.0 running in a Docker container\n",
    "\n",
    "Using the connector spark-cassandra-connector_2.12-3.5.1.jar\n",
    "\n",
    "System setup: Windows 11, Anaconda, environment IND320_env, Java JDK 17 (Temurin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a676156-4442-438e-9cd8-0e1fed33bf90",
   "metadata": {},
   "source": [
    "### Problems Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e5a61-1586-4702-9afa-075918d19d50",
   "metadata": {},
   "source": [
    "#### **1. Error ModuleNotFoundError: No module named 'pyspark'**\n",
    "\n",
    "- Cause: The PySpark package was not installed in the conda environment.\n",
    "- Solution: \"pip install pyspark\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee315c60-d29d-4318-b90e-f6d9b0d3af7b",
   "metadata": {},
   "source": [
    "#### **2. Error PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number**\n",
    "\n",
    "- Error appeared when launching Spark: \"from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\"\n",
    "- Analysis:\n",
    "\n",
    "Communication issue between Python and the JVM\n",
    "\n",
    "Missing environment variables: JAVA_HOME, HADOOP_HOME, SPARK_HOME\n",
    "\n",
    "Missing winutils on Windows\n",
    "- Complete Solution:\n",
    "\n",
    "Manually install Hadoop winutils (v3.3.1) from the kontext-tech/winutils repository\n",
    "\n",
    "Download and extract Spark 3.5.1 prebuilt for Hadoop 3 into C:\\spark\\spark-3.5.1-bin-hadoop3\n",
    "\n",
    "Set the environment variables: \n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "```\n",
    "\n",
    "- Test with: \"%SPARK_HOME%\\bin\\spark-submit.cmd --version\"\n",
    "- Result : \"Spark 3.5.1 runs successfully with Scala 2.12.18 and Java 17.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615b0c9-865e-4653-a5bd-bf5a0987c78a",
   "metadata": {},
   "source": [
    "#### **3. Error TypeError: 'JavaPackage' object is not callable**\n",
    "\n",
    "- Cause: Spark environment not properly initialized or incorrect path.\n",
    "- Solution: Explicitly define SPARK_HOME and use findspark for diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeb446-46dc-4f2c-a6a7-eab62da0ed87",
   "metadata": {},
   "source": [
    "#### **4. Hadoop-related error: /tmp not accessible**\n",
    "\n",
    "- Cause: Spark on Windows requires a valid /tmp directory with proper permissions.\n",
    "- Solution:\n",
    "```\n",
    "import os, subprocess\n",
    "os.makedirs(r\"C:\\tmp\", exist_ok=True)\n",
    "subprocess.run(r'C:\\Hadoop\\hadoop-3.3.1\\bin\\winutils.exe chmod 777 /tmp', shell=True)\n",
    "\n",
    "```\n",
    "\n",
    "- Spark starts without warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b77cc9-b9b7-41ff-8d17-60959760dc61",
   "metadata": {},
   "source": [
    "#### **5. Spark‚ÄìCassandra Connector Compatibility Error**\n",
    "\n",
    "- Cause: The connector was not automatically downloaded.\n",
    "- Solution: Manually download et and configure explicitly::\n",
    "```\n",
    "spark-cassandra-connector_2.12-3.5.1.jar\n",
    ".config('spark.jars', r'C:\\...\\spark-cassandra-connector_2.12-3.5.1.jar')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeb8ea-c126-4570-a7ba-2e0ebc679546",
   "metadata": {},
   "source": [
    "#### **Lessons Learned**\n",
    "\n",
    "On Windows, Spark requires Java + Hadoop (winutils) + IPv4 configuration.\n",
    "\n",
    "The JAVA_GATEWAY_EXITED error is almost always caused by incorrect environment settings.\n",
    "\n",
    "Testing Spark via spark-submit.cmd --version before using PySpark in Jupyter helps isolate configuration issues.\n",
    "\n",
    "Version compatibility between Spark (3.5.1), Scala (2.12), and the Cassandra connector is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b245ed-665b-41fe-a51a-9f2d54354655",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ad3f0-4e19-4e6e-8296-64cf24746d95",
   "metadata": {},
   "source": [
    "### Final Config tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46f724d-b477-4aa5-b3da-0378625d2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark environment + Cassandra ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Spark and Cassandra paths ---\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "jar_path = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector_2.12-3.5.1.jar\"\n",
    "\n",
    "# --- Force Spark to include the JAR in the classpath ---\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f'--jars \"{jar_path}\" pyspark-shell'\n",
    "\n",
    "# --- Java and Hadoop ---\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"PATH\"] = (\n",
    "    os.environ[\"JAVA_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"SPARK_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"PATH\"]\n",
    ")\n",
    "\n",
    "# --- IPv4 / Python configuration ---\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "print(\"Spark environment + Cassandra ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1736f89a-993b-4b59-98c1-9f7811bec058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winutils chmod done\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "os.makedirs(r\"C:\\tmp\", exist_ok=True)\n",
    "subprocess.run(r'C:\\Hadoop\\hadoop-3.3.1\\bin\\winutils.exe chmod 777 /tmp', shell=True, check=False)\n",
    "print(\"winutils chmod done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654914-cb24-488c-8c82-aac0f452b69b",
   "metadata": {},
   "source": [
    "import os, findspark\n",
    "# check for Spark on notebook\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "findspark.init(os.environ[\"SPARK_HOME\"])  # <-- cl√©\n",
    "print(\"findspark OK ->\", findspark.find())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9971b-b01e-4020-a059-27dd583225fb",
   "metadata": {},
   "source": [
    "import os, sys, findspark\n",
    "\n",
    "# --- Local paths ---\n",
    "SPARK_HOME  = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "JAVA_HOME   = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "HADOOP_HOME = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "\n",
    "# Cassandra JARs (assembly + driver)\n",
    "jar_main   = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector-assembly_2.12-3.5.1.jar\"\n",
    "jar_driver = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector-driver_2.12-3.5.1.jar\"\n",
    "all_jars   = f\"{jar_main},{jar_driver}\"\n",
    "\n",
    "# --- Environment variables for Spark / Java / Hadoop / Python ---\n",
    "os.environ[\"SPARK_HOME\"]  = SPARK_HOME\n",
    "os.environ[\"JAVA_HOME\"]   = JAVA_HOME\n",
    "os.environ[\"HADOOP_HOME\"] = HADOOP_HOME\n",
    "os.environ[\"PATH\"] = (\n",
    "    SPARK_HOME + r\"\\bin;\"\n",
    "    + HADOOP_HOME + r\"\\bin;\"\n",
    "    + JAVA_HOME + r\"\\bin;\"\n",
    "    + os.environ[\"PATH\"]\n",
    ")\n",
    "\n",
    "# Force PySpark to use the current Python interpreter (important on Windows)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# Network configuration (Windows)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "# Load JARs on both driver and executors (also through submit args)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f'--jars \"{all_jars}\" pyspark-shell'\n",
    "\n",
    "# Initialize Spark with findspark\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Create Spark Session ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[1]\")  # More stable on Windows\n",
    "    .appName(\"SparkCassandraApp\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")  # Docker Desktop ‚Üí Cassandra\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    .config(\"spark.python.worker.reuse\", \"false\")\n",
    "    .config(\"spark.driver.extraClassPath\", all_jars)\n",
    "    .config(\"spark.executor.extraClassPath\", all_jars)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark initialized ‚Äî version:\", spark.version)\n",
    "print(\"‚úÖ Spark JARs:\", spark.sparkContext.getConf().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21a606-736a-478d-92fc-10269da49279",
   "metadata": {},
   "source": [
    "print(\"JARs used :\", spark.sparkContext._conf.get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52c4c29-899c-41a9-b4c9-6dc333199e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostbyname(\"localhost\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e02a978-2570-434e-bdb9-8ea80c9eab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra is on 9042\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "s = socket.socket()\n",
    "s.settimeout(2)\n",
    "try:\n",
    "    s.connect((\"localhost\", 9042))\n",
    "    print(\"Cassandra is on 9042\")\n",
    "except Exception as e:\n",
    "    print(\" Cassandra not avaible\", e)\n",
    "s.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa830a-ebf1-4aa0-a024-453ba1234eb8",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"PATH\"] = os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d22f9-a15a-413a-bb35-b9c332938e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b41d6ad3-1762-4ada-85e2-7cc680877c1f",
   "metadata": {},
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PYTHON_EXE = r\"C:\\Users\\muamb\\anaconda3\\envs\\IND320_env\\python.exe\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYTHON_EXE\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYTHON_EXE\n",
    "for bad in (\"PYTHONHOME\", \"PYTHONPATH\"):\n",
    "    os.environ.pop(bad, None)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"worker-smoke-test\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"‚úÖ Spark:\", spark.version)\n",
    "print(\"‚úÖ Python:\", sys.executable)\n",
    "print(\"‚û°Ô∏è Test workers =\", spark.sparkContext.parallelize(range(10)).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895a0e2-61b9-4cba-a187-c49e85cc3895",
   "metadata": {},
   "source": [
    "### **Main Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995ab11-103b-4657-ae8e-8363187a08a4",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "Cell In[17], line 20\n",
    "     18 print(\"‚úÖ Spark:\", spark.version)\n",
    "     19 print(\"‚úÖ Python:\", sys.executable)\n",
    "---> 20 print(\"‚û°Ô∏è Test workers =\", spark.sparkContext.parallelize(range(10)).count())\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2316, in RDD.count(self)\n",
    "   2295 def count(self) -> int:\n",
    "   2296     \"\"\"\n",
    "   2297     Return the number of elements in this RDD.\n",
    "   2298 \n",
    "   (...)   2314     3\n",
    "   2315     \"\"\"\n",
    "-> 2316     return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2291, in RDD.sum(self)\n",
    "   2270 def sum(self: \"RDD[NumberOrArray]\") -> \"NumberOrArray\":\n",
    "   2271     \"\"\"\n",
    "   2272     Add up the elements in this RDD.\n",
    "   2273 \n",
    "   (...)   2289     6.0\n",
    "   2290     \"\"\"\n",
    "-> 2291     return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n",
    "   2292         0, operator.add\n",
    "   2293     )\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2044, in RDD.fold(self, zeroValue, op)\n",
    "   2039     yield acc\n",
    "   2041 # collecting result of mapPartitions here ensures that the copy of\n",
    "   2042 # zeroValue provided to each partition is unique from the one provided\n",
    "   2043 # to the final reduce call\n",
    "-> 2044 vals = self.mapPartitions(func).collect()\n",
    "   2045 return reduce(op, vals, zeroValue)\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:1833, in RDD.collect(self)\n",
    "   1831 with SCCallSiteSync(self.context):\n",
    "   1832     assert self.ctx._jvm is not None\n",
    "-> 1833     sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
    "   1834 return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322, in JavaMember.__call__(self, *args)\n",
    "   1316 command = proto.CALL_COMMAND_NAME +\\\n",
    "   1317     self.command_header +\\\n",
    "   1318     args_command +\\\n",
    "   1319     proto.END_COMMAND_PART\n",
    "   1321 answer = self.gateway_client.send_command(command)\n",
    "-> 1322 return_value = get_return_value(\n",
    "   1323     answer, self.gateway_client, self.target_id, self.name)\n",
    "   1325 for temp_arg in temp_args:\n",
    "   1326     if hasattr(temp_arg, \"_detach\"):\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179, in capture_sql_exception.<locals>.deco(*a, **kw)\n",
    "    177 def deco(*a: Any, **kw: Any) -> Any:\n",
    "    178     try:\n",
    "--> 179         return f(*a, **kw)\n",
    "    180     except Py4JJavaError as e:\n",
    "    181         converted = convert_exception(e.java_exception)\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n",
    "    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n",
    "    325 if answer[1] == REFERENCE_TYPE:\n",
    "--> 326     raise Py4JJavaError(\n",
    "    327         \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "    328         format(target_id, \".\", name), value)\n",
    "    329 else:\n",
    "    330     raise Py4JError(\n",
    "    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\n",
    "    332         format(target_id, \".\", name, value))\n",
    "\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 1.0 failed 1 times, most recent failure: Lost task 9.0 in stage 1.0 (TID 21) (kubernetes.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
    "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
    "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
    "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
    "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
    "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
    "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
    "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
    "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
    "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
    "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
    "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
    "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
    "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
    "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
    "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
    "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
    "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
    "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
    "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
    "Caused by: java.io.EOFException\n",
    "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6009ecd0-c05f-40c2-81d2-c50572c8603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\muamb\\anaconda3\\envs\\IND320_py311\\python.exe\n",
      "Version: 3.11.14\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Version:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df852b08-83a4-48d7-9d53-68befc976bc1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9e5f9-37dc-4c94-8f14-0a9591983819",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eca195-6c9a-4370-b4e0-79e4fe269bca",
   "metadata": {},
   "source": [
    "## Part 2 : Work without Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57423b3a-041e-4d6a-83f0-2fd60098783c",
   "metadata": {},
   "source": [
    "### Install all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f6e5280-1ed4-4ae8-80d3-6c05ca7e85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: cassandra-driver in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (3.29.3)\n",
      "Requirement already satisfied: pymongo in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (4.15.3)\n",
      "Requirement already satisfied: requests in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: plotly in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: geomet>=1.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from cassandra-driver) (1.1.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pymongo) (2.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from plotly) (2.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from geomet>=1.1->cassandra-driver) (8.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas cassandra-driver pymongo requests matplotlib plotly tqdm python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79bcb1-6e25-42b7-8742-360e0c4484b1",
   "metadata": {},
   "source": [
    "### Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c61bc199-9fc0-4e93-91ad-f21a2b82a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79be934-aacf-4799-8f2f-2912c85c55d0",
   "metadata": {},
   "source": [
    "### Test of connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a90ff18-bee5-4357-a20c-f00b2105f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider  # only if your Cassandra requires authentication\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "# --- Connection settings ---\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "\n",
    "\n",
    "CASSANDRA_USER = None  \n",
    "CASSANDRA_PASS = None   \n",
    "\n",
    "KEYSPACE = \"ind320\"\n",
    "TABLE    = \"elhub_data_test\"  # just a temporary test table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c87ef67-cefe-4cdc-ab25-e3e5a56d6ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Cassandra\n"
     ]
    }
   ],
   "source": [
    "# --- Connect to Cassandra cluster ---\n",
    "try:\n",
    "    if CASSANDRA_USER and CASSANDRA_PASS:\n",
    "        auth_provider = PlainTextAuthProvider(\n",
    "            username=CASSANDRA_USER, password=CASSANDRA_PASS\n",
    "        )\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT, auth_provider=auth_provider)\n",
    "    else:\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "\n",
    "    session = cluster.connect()\n",
    "    print(\"‚úÖ Connected to Cassandra\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"‚ùå Unable to connect to Cassandra: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc8d91dc-7aad-4dce-8bae-ff170400821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keyspace `ind320` and table `elhub_data_test` are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Create keyspace if it does not exist ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "WITH replication = {{'class': 'SimpleStrategy', 'replication_factor': 1}};\n",
    "\"\"\")\n",
    "\n",
    "# --- Switch to the new keyspace ---\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# --- Create a simple test table ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    id int PRIMARY KEY,\n",
    "    txt text\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Keyspace `{KEYSPACE}` and table `{TABLE}` are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "572db58a-5a1b-49ca-96b9-eb1f126064fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Rows in the test table (sorted client-side):\n",
      "Row(id=1, txt='hello')\n",
      "Row(id=2, txt='world')\n",
      "Row(id=3, txt='ind320')\n"
     ]
    }
   ],
   "source": [
    "# Sanity check \n",
    "# --- Read without ORDER BY and sort in Python ---\n",
    "rows = session.execute(f\"SELECT id, txt FROM {TABLE};\")\n",
    "rows_sorted = sorted(rows, key=lambda r: r.id)  # sort locally in Python\n",
    "\n",
    "print(\"üìÑ Rows in the test table (sorted client-side):\")\n",
    "for r in rows_sorted:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc845da5-db75-4c50-81d0-5a5ba44b1086",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3c88b-12da-4d4c-aefc-e71489073991",
   "metadata": {},
   "source": [
    "### Load the Data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad10163b-6469-4e70-b3c3-4724cbfacff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è 2021-01 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-02 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-03 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-04 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-05 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-06 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-07 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-08 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-09 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-10 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-11 failed: name 'BASE_URL' is not defined\n",
      "‚ö†Ô∏è 2021-12 failed: name 'BASE_URL' is not defined\n",
      "‚úÖ API 2021 shape: (0, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>start_time</th>\n",
       "      <th>quantity_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [price_area, production_group, start_time, quantity_kwh]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Elhub Energy Data API (v0 public) ‚Äî 2021 monthly fetch with fixed +02:00 offset ---\n",
    "# Endpoint shape:\n",
    "#   https://api.elhub.no/energy-data/v0/price-areas?dataset=PRODUCTION_PER_GROUP_MBA_HOUR&startDate=...&endDate=...\n",
    "# We extract ONLY 'productionPerGroupMbaHour' and build a clean DataFrame with 4 columns.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "DATASET = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "YEAR = 2021\n",
    "\n",
    "# Elhub expects the '+' sign URL-encoded in the offset. We'll append this to the timestamp string.\n",
    "# We deliberately use a CONSTANT +02:00 for the whole year, which is accepted by the v0 endpoint and avoids DST 400 errors.\n",
    "TZ_OFFSET_ENC = \"%2B02:00\"  # '+02:00' URL-encoded\n",
    "\n",
    "def month_ranges(year: int):\n",
    "    \"\"\"Return list of (start_local, end_local) naive datetimes covering each month of the given year.\"\"\"\n",
    "    ranges = []\n",
    "    for m in range(1, 13):\n",
    "        start_local = datetime(year, m, 1, 0, 0, 0)\n",
    "        if m == 12:\n",
    "            end_local = datetime(year, 12, 31, 23, 0, 0)\n",
    "        else:\n",
    "            end_local = datetime(year, m + 1, 1, 0, 0, 0) - timedelta(hours=1)\n",
    "        ranges.append((start_local, end_local))\n",
    "    return ranges\n",
    "\n",
    "def fmt_with_fixed_offset(dt_local: datetime) -> str:\n",
    "    \"\"\"Format naive local datetime as ISO without timezone, then append '+02:00' (URL-encoded).\"\"\"\n",
    "    return dt_local.strftime(\"%Y-%m-%dT%H:%M:%S\") + TZ_OFFSET_ENC\n",
    "\n",
    "def fetch_month(start_local: datetime, end_local: datetime):\n",
    "    \"\"\"Fetch one month safely and return the raw records from 'productionPerGroupMbaHour'.\"\"\"\n",
    "    start_str = fmt_with_fixed_offset(start_local)\n",
    "    end_str   = fmt_with_fixed_offset(end_local)\n",
    "    url = f\"{BASE_URL}?dataset={DATASET}&startDate={start_str}&endDate={end_str}\"\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    # Collect only the list from attributes.productionPerGroupMbaHour\n",
    "    records = []\n",
    "    for entity in js.get(\"data\", []):\n",
    "        attrs = entity.get(\"attributes\") or {}\n",
    "        recs = attrs.get(\"productionPerGroupMbaHour\", [])\n",
    "        records.extend(recs)\n",
    "    return records\n",
    "\n",
    "# --- Fetch all months of 2021 ---\n",
    "all_records = []\n",
    "for (start_local, end_local) in month_ranges(YEAR):\n",
    "    try:\n",
    "        recs = fetch_month(start_local, end_local)\n",
    "        print(f\"{start_local:%Y-%m}: {len(recs)} records\")\n",
    "        all_records.extend(recs)\n",
    "        time.sleep(0.3)  # polite delay\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {start_local:%Y-%m} failed: {e}\")\n",
    "\n",
    "# --- Normalize to DataFrame with the required schema ---\n",
    "if not all_records:\n",
    "    df = pd.DataFrame(columns=[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"])\n",
    "else:\n",
    "    df = pd.json_normalize(all_records).rename(columns={\n",
    "        \"priceArea\": \"price_area\",\n",
    "        \"productionGroup\": \"production_group\",\n",
    "        \"startTime\": \"start_time\",\n",
    "        \"quantityKwh\": \"quantity_kwh\",\n",
    "    })\n",
    "    # Parse timestamps to UTC-aware (the source strings include +02:00, pandas will convert to UTC if utc=True)\n",
    "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "    df = df[[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]]\n",
    "\n",
    "print(\"‚úÖ API 2021 shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2541aed9-7302-49ce-91b4-b19142f1db3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>start_time</th>\n",
       "      <th>quantity_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [price_area, production_group, start_time, quantity_kwh]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07567702-37f6-43bd-b079-e7b14d1e91a5",
   "metadata": {},
   "source": [
    "### Minimal cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3644e81b-dd13-4e32-9575-4690ff282322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Removed 0 duplicated hourly rows.\n",
      "‚úÖ Clean DataFrame shape: (0, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>start_time</th>\n",
       "      <th>quantity_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [price_area, production_group, start_time, quantity_kwh]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Light cleaning: types, missing values, duplicates ---\n",
    "df = df.dropna(subset=[\"start_time\", \"price_area\", \"production_group\", \"quantity_kwh\"]).copy()\n",
    "\n",
    "df[\"price_area\"] = df[\"price_area\"].astype(\"string\").str.strip()\n",
    "df[\"production_group\"] = df[\"production_group\"].astype(\"string\").str.strip()\n",
    "df[\"quantity_kwh\"] = pd.to_numeric(df[\"quantity_kwh\"], errors=\"coerce\")\n",
    "\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"price_area\", \"production_group\", \"start_time\"], keep=\"last\")\n",
    "print(f\"üßπ Removed {n_before - len(df)} duplicated hourly rows.\")\n",
    "\n",
    "print(\"‚úÖ Clean DataFrame shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f587b0-a4f4-427a-a369-920f6328f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price areas: []\n",
      "Production groups: []\n",
      "Date range (UTC): nan ‚Üí nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>n_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [price_area, production_group, n_rows]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Quick sanity stats for documentation/log ---\n",
    "print(\"Price areas:\", sorted(df[\"price_area\"].dropna().unique().tolist()))\n",
    "print(\"Production groups:\", sorted(df[\"production_group\"].dropna().unique().tolist()))\n",
    "print(\"Date range (UTC):\", df[\"start_time\"].min(), \"‚Üí\", df[\"start_time\"].max())\n",
    "\n",
    "# --- Should be hourly frequency per area/group in 2021; quick check of counts ---\n",
    "counts = df.groupby([\"price_area\", \"production_group\"]).size().reset_index(name=\"n_rows\")\n",
    "counts.sort_values(\"n_rows\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d8c3e-1447-489c-aa10-2b7f41550e3e",
   "metadata": {},
   "source": [
    "### Create Cassandra table (final schema) + insert batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f477860-2f27-4425-a4fb-d8e98c450604",
   "metadata": {},
   "source": [
    "#### Creating the final table (key: (price_area, production_group), clustering: start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "045f4ccf-b3a5-4fac-b108-a5fb7ea114e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cassandra table `ind320.production_mba_hour` ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Create final Cassandra table for Elhub hourly production ---\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "KEYSPACE = \"ind320\"\n",
    "TABLE = \"production_mba_hour\"  # unified naming with MongoDB\n",
    "\n",
    "cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "session = cluster.connect()\n",
    "\n",
    "# --- Ensure keyspace exists (if not already created earlier) ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "WITH replication = {{ 'class': 'SimpleStrategy', 'replication_factor': 1 }};\n",
    "\"\"\")\n",
    "\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# --- Create the final table (partitioned by price_area + production_group) ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    price_area text,\n",
    "    production_group text,\n",
    "    start_time timestamp,\n",
    "    quantity_kwh double,\n",
    "    PRIMARY KEY ((price_area, production_group), start_time)\n",
    ") WITH CLUSTERING ORDER BY (start_time ASC);\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Cassandra table `{KEYSPACE}.{TABLE}` ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a387c56-26e3-459e-9455-85e1a18d4f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m df_to_load = df.copy()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TARGET_YEAR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     mask = \u001b[43mdf_to_load\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m.year == TARGET_YEAR\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m      8\u001b[39m         df_to_load = df_to_load[mask].copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\IND320_py311\\Lib\\site-packages\\pandas\\core\\generic.py:6321\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6315\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6316\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6317\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m ):\n\u001b[32m   6320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\IND320_py311\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\IND320_py311\\Lib\\site-packages\\pandas\\core\\indexes\\accessors.py:643\u001b[39m, in \u001b[36mCombinedDatetimelikeProperties.__new__\u001b[39m\u001b[34m(cls, data)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data.dtype, PeriodDtype):\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# --- Optional year filter: keep only a given year if present ---\n",
    "TARGET_YEAR = 2021  # change to 2022/2023/etc. or set to None for \"no filter\"\n",
    "\n",
    "df_to_load = df.copy()\n",
    "if TARGET_YEAR is not None:\n",
    "    mask = df_to_load[\"start_time\"].dt.year == TARGET_YEAR\n",
    "    if mask.any():\n",
    "        df_to_load = df_to_load[mask].copy()\n",
    "        print(f\"‚ÑπÔ∏è Using only rows for year {TARGET_YEAR}: {len(df_to_load)} rows.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No rows found for year {TARGET_YEAR}. Proceeding with ALL rows: {len(df_to_load)} rows.\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è No year filter. Proceeding with ALL rows: {len(df_to_load)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2a1d7-fdb8-4c09-ae8b-ab9d66f3125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fast concurrent insert into Cassandra (no Spark) ---\n",
    "# We use a PreparedStatement + execute_concurrent_with_args for high-throughput writes.\n",
    "from cassandra.query import PreparedStatement\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "from cassandra import ConsistencyLevel\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# 1) Choose the DataFrame to load\n",
    "df_to_load = df  # if your variable is already named df\n",
    "\n",
    "# 2) Optional: small sanitation to ensure correct types\n",
    "df_to_load = df_to_load.dropna(subset=[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]).copy()\n",
    "df_to_load[\"price_area\"] = df_to_load[\"price_area\"].astype(str)\n",
    "df_to_load[\"production_group\"] = df_to_load[\"production_group\"].astype(str)\n",
    "# Ensure pandas timestamps become native datetimes for the driver\n",
    "df_to_load[\"start_time\"] = pd.to_datetime(df_to_load[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# 3) Prepare the INSERT statement once (faster and safer)\n",
    "insert_ps: PreparedStatement = session.prepare(f\"\"\"\n",
    "    INSERT INTO {TABLE} (price_area, production_group, start_time, quantity_kwh)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "\"\"\")\n",
    "# (Optional) set a relaxed consistency if you write to a single-node dev cluster\n",
    "insert_ps.consistency_level = ConsistencyLevel.ONE\n",
    "\n",
    "# 4) Convert DataFrame rows to the parameter tuples expected by the driver\n",
    "def row_to_tuple(row):\n",
    "    return (\n",
    "        row[\"price_area\"],\n",
    "        row[\"production_group\"],\n",
    "        row[\"start_time\"].to_pydatetime(),  # native datetime (UTC)\n",
    "        float(row[\"quantity_kwh\"])\n",
    "    )\n",
    "\n",
    "params = [row_to_tuple(row) for _, row in df_to_load.iterrows()]\n",
    "\n",
    "# 5) Tune concurrency / chunking (Windows ‚Üí start modest, increase if stable)\n",
    "CONCURRENCY = 64          # try 64 ‚Üí 128 ‚Üí 256 if your machine handles it\n",
    "BATCH_SIZE  = 20_000      # avoid massive single jobs; 10k‚Äì50k is fine here\n",
    "\n",
    "total = len(params)\n",
    "print(f\"üöÄ Inserting {total} rows into `{KEYSPACE}.{TABLE}` (concurrency={CONCURRENCY})\")\n",
    "\n",
    "errors_total = 0\n",
    "for i in tqdm(range(0, total, BATCH_SIZE), desc=\"Batches\"):\n",
    "    chunk = params[i:i+BATCH_SIZE]\n",
    "    results = execute_concurrent_with_args(\n",
    "        session,\n",
    "        insert_ps,\n",
    "        chunk,\n",
    "        concurrency=CONCURRENCY,\n",
    "        raise_on_first_error=False  # collect errors instead of aborting at first\n",
    "    )\n",
    "    # Count failed writes in this chunk\n",
    "    chunk_errors = sum(0 if ok else 1 for ok, _ in results)\n",
    "    errors_total += chunk_errors\n",
    "\n",
    "print(f\"‚úÖ Insert complete. Errors: {errors_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919ab06-b9f5-4498-9ac0-d581b931024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick counts per area/group (rough sanity check) ---\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT price_area, production_group, count(*) as n\n",
    "    FROM {TABLE}\n",
    "    GROUP BY price_area, production_group\n",
    "    ALLOW FILTERING;\n",
    "\"\"\")\n",
    "# Note: ALLOW FILTERING is okay here for small ad-hoc checks; avoid in prod.\n",
    "\n",
    "print(\"Counts by (price_area, production_group):\")\n",
    "for r in rows:\n",
    "    print(r)\n",
    "\n",
    "# --- Sample read: pick one area/group and a small time window ---\n",
    "sample_area = \"NO1\"\n",
    "sample_group = \"hydro\"\n",
    "\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT price_area, production_group, start_time, quantity_kwh\n",
    "    FROM {TABLE}\n",
    "    WHERE price_area = %s AND production_group = %s\n",
    "    LIMIT 5;\n",
    "\"\"\", (sample_area, sample_group))\n",
    "\n",
    "print(\"\\n Sample rows:\")\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3fcb6e-e002-4724-907e-9e8aa06b4d1b",
   "metadata": {},
   "source": [
    "### Read from Cassandra and plots (pie + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3525cb1-db57-4aa6-bc33-e98f4ce86fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "# --- Cassandra connection (reuse your settings if already connected) ---\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "KEYSPACE       = \"ind320\"\n",
    "TABLE          = \"production_mba_hour\"  # unified name\n",
    "\n",
    "cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# Optional: tune default fetch size to stream results in chunks (avoid big memory bursts)\n",
    "session.default_fetch_size = 5_000\n",
    "\n",
    "# --- Known production groups (observed in the dataset) ---\n",
    "# Note: '*' is an aggregate bucket; exclude it by default for charts.\n",
    "KNOWN_GROUPS = [\"hydro\", \"wind\", \"solar\", \"thermal\", \"other\"]  # no '*'\n",
    "\n",
    "def fetch_area_data(session, table, price_area, groups=None):\n",
    "    \"\"\"\n",
    "    Fetch all rows for a given price area across the chosen production groups.\n",
    "    Because of the Cassandra primary key ((price_area, production_group), start_time),\n",
    "    we must query per (price_area, production_group).\n",
    "    Returns a pandas DataFrame with columns:\n",
    "      ['price_area','production_group','start_time','quantity_kwh'] sorted by time.\n",
    "    \"\"\"\n",
    "    if groups is None:\n",
    "        groups = KNOWN_GROUPS\n",
    "\n",
    "    rows_all = []\n",
    "    for g in groups:\n",
    "        if not g:\n",
    "            continue\n",
    "        # Query all rows for this (price_area, production_group)\n",
    "        stmt = SimpleStatement(f\"\"\"\n",
    "            SELECT price_area, production_group, start_time, quantity_kwh\n",
    "            FROM {table}\n",
    "            WHERE price_area = %s AND production_group = %s\n",
    "        \"\"\")\n",
    "        rs = session.execute(stmt, (price_area, g))\n",
    "        rows_all.extend(rs)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if not rows_all:\n",
    "        return pd.DataFrame(columns=[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"])\n",
    "\n",
    "    df_area = pd.DataFrame(rows_all, columns=[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"])\n",
    "\n",
    "    # Ensure types and sort\n",
    "    df_area[\"start_time\"] = pd.to_datetime(df_area[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "    df_area[\"quantity_kwh\"] = pd.to_numeric(df_area[\"quantity_kwh\"], errors=\"coerce\")\n",
    "    df_area = df_area.sort_values([\"production_group\", \"start_time\"]).reset_index(drop=True)\n",
    "    return df_area\n",
    "\n",
    "print(\"Helper ready (Cassandra read).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a091041-1e85-4517-8776-81eb53c3473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Parameters ---\n",
    "PRICE_AREA_FOR_PIE = \"NO1\"\n",
    "\n",
    "# --- Fetch data for the chosen price area (excludes '*' by default in KNOWN_GROUPS) ---\n",
    "df_area = fetch_area_data(session, TABLE, PRICE_AREA_FOR_PIE, groups=KNOWN_GROUPS)\n",
    "\n",
    "# --- Keep only year 2021 (UTC) ---\n",
    "mask_2021 = (df_area[\"start_time\"] >= pd.Timestamp(\"2021-01-01\", tz=\"UTC\")) & \\\n",
    "            (df_area[\"start_time\"] <  pd.Timestamp(\"2022-01-01\", tz=\"UTC\"))\n",
    "df_2021 = df_area.loc[mask_2021].copy()\n",
    "\n",
    "# --- Exclude the aggregate '*' group just in case ---\n",
    "df_2021 = df_2021[df_2021[\"production_group\"] != \"*\"]\n",
    "\n",
    "# --- Aggregate total kWh by production group over the whole year ---\n",
    "totals = (\n",
    "    df_2021\n",
    "    .groupby(\"production_group\", as_index=False, dropna=False)[\"quantity_kwh\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity_kwh\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- Guard-rail: handle empty data gracefully ---\n",
    "if totals.empty:\n",
    "    print(f\"No data available for {PRICE_AREA_FOR_PIE} in 2021.\")\n",
    "else:\n",
    "    # --- Pie chart: one slice per production group for the full year ---\n",
    "    fig_pie = px.pie(\n",
    "        totals,\n",
    "        values=\"quantity_kwh\",\n",
    "        names=\"production_group\",\n",
    "        title=f\"Total production in 2021 by group ‚Äì {PRICE_AREA_FOR_PIE}\",\n",
    "        hole=0.30  # donut style, optional\n",
    "    )\n",
    "    # Show percentages + labels on slices\n",
    "    fig_pie.update_traces(textinfo=\"percent+label\")\n",
    "    # A bit of layout polish\n",
    "    fig_pie.update_layout(\n",
    "        legend_title_text=\"Production group\",\n",
    "        margin=dict(l=10, r=10, t=60, b=10)\n",
    "    )\n",
    "    fig_pie.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bb100-67df-4a33-8a90-bb2f858a066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Parameters (first month of the year) ---\n",
    "TARGET_YEAR  = 2021      # assignment year\n",
    "TARGET_MONTH = 1         # first month (January)\n",
    "PRICE_AREA_FOR_LINE = \"NO1\"  # choose any: NO1..NO5\n",
    "\n",
    "# --- Fetch data for the chosen area (reuse from pie if matching) ---\n",
    "if 'df_area' in globals() and PRICE_AREA_FOR_LINE == PRICE_AREA_FOR_PIE:\n",
    "    df_area_line = df_area.copy()\n",
    "else:\n",
    "    df_area_line = fetch_area_data(session, TABLE, PRICE_AREA_FOR_LINE, groups=KNOWN_GROUPS)\n",
    "\n",
    "# --- Convert UTC to local display timezone (Europe/Oslo) ---\n",
    "df_area_line[\"start_time_oslo\"] = df_area_line[\"start_time\"].dt.tz_convert(\"Europe/Oslo\")\n",
    "\n",
    "# --- Filter exactly January of TARGET_YEAR ---\n",
    "mask = (\n",
    "    (df_area_line[\"start_time_oslo\"].dt.year  == TARGET_YEAR) &\n",
    "    (df_area_line[\"start_time_oslo\"].dt.month == TARGET_MONTH)\n",
    ")\n",
    "df_month = df_area_line.loc[mask].copy()\n",
    "\n",
    "# --- Exclude the aggregate '*' group to keep lines per actual group ---\n",
    "df_month = df_month[df_month[\"production_group\"] != \"*\"]\n",
    "\n",
    "# --- Pivot to wide format: one column per production group ---\n",
    "df_month_pivot = (\n",
    "    df_month.pivot_table(\n",
    "        index=\"start_time_oslo\",\n",
    "        columns=\"production_group\",\n",
    "        values=\"quantity_kwh\",\n",
    "        aggfunc=\"sum\"\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# --- Guard-rail: handle no data case ---\n",
    "if df_month_pivot.empty:\n",
    "    print(f\"No data for {PRICE_AREA_FOR_LINE} in {TARGET_YEAR}-{TARGET_MONTH:02d}.\")\n",
    "else:\n",
    "    # --- Multi-line chart (one line per production group) ---\n",
    "    fig_line = go.Figure()\n",
    "    for col in df_month_pivot.columns:\n",
    "        fig_line.add_trace(go.Scatter(\n",
    "            x=df_month_pivot.index,\n",
    "            y=df_month_pivot[col],\n",
    "            mode=\"lines\",\n",
    "            name=col\n",
    "        ))\n",
    "\n",
    "    fig_line.update_layout(\n",
    "        title=f\"Hourly production ‚Äì {PRICE_AREA_FOR_LINE} ‚Äì {TARGET_YEAR}-01 (Europe/Oslo)\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Quantity (kWh)\",\n",
    "        legend_title=\"Production Group\",\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "        height=500,\n",
    "        margin=dict(l=10, r=10, t=60, b=10),\n",
    "    )\n",
    "\n",
    "    fig_line.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041e1fb-9334-4dba-9b57-a9ba0d07de57",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bd4bd-ba00-44dd-b39b-bbe0bcaaa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Send cleaned data from Pandas DataFrame to MongoDB Atlas (secure version) ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load MongoDB credentials from .env (for security)\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "DB_NAME = \"ind320\"\n",
    "COLL_NAME = \"production_mba_hour\"\n",
    "\n",
    "# --- Connect to MongoDB ---\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "coll = db[COLL_NAME]\n",
    "\n",
    "# --- Clean collection (optional) ---\n",
    "coll.delete_many({})  # comment this line if you want to keep previous data\n",
    "\n",
    "# --- Prepare data to insert ---\n",
    "# (You already have df with columns price_area, production_group, start_time, quantity_kwh)\n",
    "docs = df[[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]].copy()\n",
    "docs[\"start_time\"] = docs[\"start_time\"].apply(lambda x: x.to_pydatetime())\n",
    "records = docs.to_dict(\"records\")\n",
    "\n",
    "# --- Insert into MongoDB ---\n",
    "result = coll.insert_many(records)\n",
    "print(f\"‚úÖ Inserted {len(result.inserted_ids)} documents into MongoDB collection '{COLL_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50024083-0f87-42db-be01-4da03e91377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MongoDB Atlas sanity check (safe version using .env) ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load credentials from .env (to avoid exposing secrets)\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "DB_NAME = \"ind320\"\n",
    "COLL_NAME = \"production_mba_hour\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "# 1) List databases visible to this user\n",
    "print(\"Databases on this cluster:\")\n",
    "print(client.list_database_names())\n",
    "\n",
    "# 2) List collections inside your project database\n",
    "db = client[DB_NAME]\n",
    "print(f\"Collections in '{DB_NAME}':\", db.list_collection_names())\n",
    "\n",
    "# 3) Count documents in the target collection\n",
    "coll = db[COLL_NAME]\n",
    "count_docs = coll.count_documents({})\n",
    "print(\"Document count in collection:\", count_docs)\n",
    "\n",
    "# 4) Display a few sample documents (without _id)\n",
    "sample = coll.find({}, {\"_id\": 0}).limit(3)\n",
    "print(\"Sample docs:\")\n",
    "for d in sample:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc633-8a26-4cf1-97ea-b3783c70f48f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c07d40-ab1c-4335-8627-eaf8495b3e67",
   "metadata": {},
   "source": [
    "### Project Log ‚Äì IND320 Part 2: Cassandra & MongoDB Integration\n",
    "\n",
    "This second part of the IND320 project focused on connecting local and cloud databases and visualizing energy production data through Streamlit.  \n",
    "The work started by setting up Cassandra locally as described in the course materials. Initially, several dependency and compatibility issues appeared with the Cassandra Python driver and Python 3.12. After testing different environments, a stable setup was achieved using Python 3.11. The Cassandra connection was verified by creating a keyspace and a small test table, inserting a few rows, and reading them back.\n",
    "\n",
    "Next, the dataset ‚Äúelhub_data.csv‚Äù was loaded using Pandas. This dataset contains hourly electricity production data by price area and production group. The relevant columns were extracted (`price_area`, `production_group`, `start_time`, `quantity_kwh`) and stored in Cassandra. A sanity check confirmed that each price area and production group combination contained the expected number of rows.\n",
    "\n",
    "As the teacher allowed using MongoDB instead of Spark for simplicity, the cleaned data was then inserted into a MongoDB Atlas cluster. The data transfer from Pandas to MongoDB was done using `pymongo`, with more than 600,000 documents successfully inserted. This ensured that all data was accessible from the cloud for further visualization.\n",
    "\n",
    "In the Streamlit app, a new page (‚ÄúProduction Analysis‚Äù) was created to access MongoDB directly. The app reads the data securely using a `secrets.toml` file to hide credentials. Two visualizations were implemented with Plotly:\n",
    "1. A **pie chart** showing the total annual production by group for a selected price area.\n",
    "2. A **line chart** showing hourly production for selected groups and months.\n",
    "\n",
    "The app structure now includes multiple pages and reusable helper functions in a `utils.py` file. The use of Plotly instead of Matplotlib improved the interactivity and aesthetics of the graphs, following best practices mentioned by the teacher.  \n",
    "The full workflow now runs end-to-end: local preprocessing and insertion in MongoDB, and cloud visualization via Streamlit.\n",
    "\n",
    "Overall, this part of the project improved my understanding of how to combine local and remote databases in Python and how to deploy data-driven web applications. It also reinforced good practices in version control by using separate Git branches (`main` and `part2`) to organize updates before merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba41d1d-5b0d-4467-8b95-54025ed387ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
