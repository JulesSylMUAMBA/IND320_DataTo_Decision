{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250abd8c-353f-4c66-b0c6-4a8bec30bdc8",
   "metadata": {},
   "source": [
    "# IND320 – Project Part 2 : Elhub Data Analysis\n",
    "## *Welcome to my Project work, part 2 - Data Sources*\n",
    "### Author : Jules Sylvain MUAMBA MVELE\n",
    "### Description :\n",
    "This notebook demonstrates the Elhub data analysis workflow for 2021.\n",
    "It includes :\n",
    "- Data ingestion from Elhub (CSV or API)\n",
    "- Storage in Cassandra and MongoDB\n",
    "- Visualizations (pie chart and line chart)\n",
    "- Documentation of AI assistance and work log\n",
    "\n",
    "**!! I created a nez branch named part2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422c6d3-f042-4ace-ad63-250d2a445704",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "- **GitHub Repository:** [https://github.com/JulesSylMUAMBA/IND320_DataTo_Decision/tree/part2](https://github.com/JulesSylMUAMBA/IND320_DataTo_Decision/tree/part2)\n",
    "- **Streamlit App:** https://ind320datatodecision-m8z5c8kxibzrnoy2parjfq.streamlit.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9307a-fed1-4eef-ab98-11ecd67927ef",
   "metadata": {},
   "source": [
    "## AI usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a46c90-5993-46d3-a2ee-0fb0b03fb593",
   "metadata": {},
   "source": [
    "During this project, I used AI tools (mainly ChatGPT) as a programming assistant and technical explainer.  \n",
    "The main tasks where AI was used include:\n",
    "- Debugging issues with Python environments (especially Cassandra and package dependencies).\n",
    "- Translating comments and explanations from French to English.\n",
    "- Writing clearly commented code that can be understood by others.\n",
    "- Structuring the project pipeline (Cassandra → MongoDB → Streamlit).\n",
    "- Clarifying the course requirements and how to implement them in a practical way.\n",
    "\n",
    "The AI was **not used to automatically generate the final results**, but as a guide to understand, fix, and organize the code.  \n",
    "All code was tested and adapted manually.  \n",
    "This use of AI helped improve productivity and understanding without replacing independent work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed507f30-c75b-457b-8e1d-d33395412665",
   "metadata": {},
   "source": [
    "## Part 1 : Diagnostic and Troubleshooting – Spark & Cassandra Configuration on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedbde8-9eb2-4e23-9e4c-87fc70c3d840",
   "metadata": {},
   "source": [
    "During the setup of the Spark + Cassandra environment, several technical issues were encountered.\n",
    "This section documents the problems, the attempted fixes, and the final working configuration, it's not working anyway, i'm open to any help.\n",
    "I tried a lot of things for days, hope it's clear enough to understand (even if it's not clear for me anymore haha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887e792-7072-460b-a19d-03c281a4b986",
   "metadata": {},
   "source": [
    "### Initial Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a91852-b149-4a9b-af6e-27506b2e9b8b",
   "metadata": {},
   "source": [
    "The goal was to run a Jupyter (Python) notebook capable of:\n",
    "\n",
    "Starting a local Spark 3.5.1 session\n",
    "\n",
    "Connecting to Cassandra 5.0 running in a Docker container\n",
    "\n",
    "Using the connector spark-cassandra-connector_2.12-3.5.1.jar\n",
    "\n",
    "System setup: Windows 11, Anaconda, environment IND320_env, Java JDK 17 (Temurin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a676156-4442-438e-9cd8-0e1fed33bf90",
   "metadata": {},
   "source": [
    "### Problems Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e5a61-1586-4702-9afa-075918d19d50",
   "metadata": {},
   "source": [
    "#### **1. Error ModuleNotFoundError: No module named 'pyspark'**\n",
    "\n",
    "- Cause: The PySpark package was not installed in the conda environment.\n",
    "- Solution: \"pip install pyspark\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee315c60-d29d-4318-b90e-f6d9b0d3af7b",
   "metadata": {},
   "source": [
    "#### **2. Error PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number**\n",
    "\n",
    "- Error appeared when launching Spark: \"from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\"\n",
    "- Analysis:\n",
    "\n",
    "Communication issue between Python and the JVM\n",
    "\n",
    "Missing environment variables: JAVA_HOME, HADOOP_HOME, SPARK_HOME\n",
    "\n",
    "Missing winutils on Windows\n",
    "- Complete Solution:\n",
    "\n",
    "Manually install Hadoop winutils (v3.3.1) from the kontext-tech/winutils repository\n",
    "\n",
    "Download and extract Spark 3.5.1 prebuilt for Hadoop 3 into C:\\spark\\spark-3.5.1-bin-hadoop3\n",
    "\n",
    "Set the environment variables: \n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "```\n",
    "\n",
    "- Test with: \"%SPARK_HOME%\\bin\\spark-submit.cmd --version\"\n",
    "- Result : \"Spark 3.5.1 runs successfully with Scala 2.12.18 and Java 17.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615b0c9-865e-4653-a5bd-bf5a0987c78a",
   "metadata": {},
   "source": [
    "#### **3. Error TypeError: 'JavaPackage' object is not callable**\n",
    "\n",
    "- Cause: Spark environment not properly initialized or incorrect path.\n",
    "- Solution: Explicitly define SPARK_HOME and use findspark for diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeb446-46dc-4f2c-a6a7-eab62da0ed87",
   "metadata": {},
   "source": [
    "#### **4. Hadoop-related error: /tmp not accessible**\n",
    "\n",
    "- Cause: Spark on Windows requires a valid /tmp directory with proper permissions.\n",
    "- Solution:\n",
    "```\n",
    "import os, subprocess\n",
    "os.makedirs(r\"C:\\tmp\", exist_ok=True)\n",
    "subprocess.run(r'C:\\Hadoop\\hadoop-3.3.1\\bin\\winutils.exe chmod 777 /tmp', shell=True)\n",
    "\n",
    "```\n",
    "\n",
    "- Spark starts without warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b77cc9-b9b7-41ff-8d17-60959760dc61",
   "metadata": {},
   "source": [
    "#### **5. Spark–Cassandra Connector Compatibility Error**\n",
    "\n",
    "- Cause: The connector was not automatically downloaded.\n",
    "- Solution: Manually download et and configure explicitly::\n",
    "```\n",
    "spark-cassandra-connector_2.12-3.5.1.jar\n",
    ".config('spark.jars', r'C:\\...\\spark-cassandra-connector_2.12-3.5.1.jar')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeb8ea-c126-4570-a7ba-2e0ebc679546",
   "metadata": {},
   "source": [
    "#### **Lessons Learned**\n",
    "\n",
    "On Windows, Spark requires Java + Hadoop (winutils) + IPv4 configuration.\n",
    "\n",
    "The JAVA_GATEWAY_EXITED error is almost always caused by incorrect environment settings.\n",
    "\n",
    "Testing Spark via spark-submit.cmd --version before using PySpark in Jupyter helps isolate configuration issues.\n",
    "\n",
    "Version compatibility between Spark (3.5.1), Scala (2.12), and the Cassandra connector is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b245ed-665b-41fe-a51a-9f2d54354655",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ad3f0-4e19-4e6e-8296-64cf24746d95",
   "metadata": {},
   "source": [
    "### Final Config tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46f724d-b477-4aa5-b3da-0378625d2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark environment + Cassandra ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Spark and Cassandra paths ---\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "jar_path = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector_2.12-3.5.1.jar\"\n",
    "\n",
    "# --- Force Spark to include the JAR in the classpath ---\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f'--jars \"{jar_path}\" pyspark-shell'\n",
    "\n",
    "# --- Java and Hadoop ---\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"PATH\"] = (\n",
    "    os.environ[\"JAVA_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"HADOOP_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"SPARK_HOME\"] + r\"\\bin;\"\n",
    "    + os.environ[\"PATH\"]\n",
    ")\n",
    "\n",
    "# --- IPv4 / Python configuration ---\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "print(\"Spark environment + Cassandra ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1736f89a-993b-4b59-98c1-9f7811bec058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winutils chmod done\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "os.makedirs(r\"C:\\tmp\", exist_ok=True)\n",
    "subprocess.run(r'C:\\Hadoop\\hadoop-3.3.1\\bin\\winutils.exe chmod 777 /tmp', shell=True, check=False)\n",
    "print(\"winutils chmod done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654914-cb24-488c-8c82-aac0f452b69b",
   "metadata": {},
   "source": [
    "import os, findspark\n",
    "# check for Spark on notebook\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "findspark.init(os.environ[\"SPARK_HOME\"])  # <-- clé\n",
    "print(\"findspark OK ->\", findspark.find())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9971b-b01e-4020-a059-27dd583225fb",
   "metadata": {},
   "source": [
    "import os, sys, findspark\n",
    "\n",
    "# --- Local paths ---\n",
    "SPARK_HOME  = r\"C:\\spark\\spark-3.5.1-bin-hadoop3\"\n",
    "JAVA_HOME   = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.13.11-hotspot\"\n",
    "HADOOP_HOME = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "\n",
    "# Cassandra JARs (assembly + driver)\n",
    "jar_main   = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector-assembly_2.12-3.5.1.jar\"\n",
    "jar_driver = r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\spark-cassandra-connector-driver_2.12-3.5.1.jar\"\n",
    "all_jars   = f\"{jar_main},{jar_driver}\"\n",
    "\n",
    "# --- Environment variables for Spark / Java / Hadoop / Python ---\n",
    "os.environ[\"SPARK_HOME\"]  = SPARK_HOME\n",
    "os.environ[\"JAVA_HOME\"]   = JAVA_HOME\n",
    "os.environ[\"HADOOP_HOME\"] = HADOOP_HOME\n",
    "os.environ[\"PATH\"] = (\n",
    "    SPARK_HOME + r\"\\bin;\"\n",
    "    + HADOOP_HOME + r\"\\bin;\"\n",
    "    + JAVA_HOME + r\"\\bin;\"\n",
    "    + os.environ[\"PATH\"]\n",
    ")\n",
    "\n",
    "# Force PySpark to use the current Python interpreter (important on Windows)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# Network configuration (Windows)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "# Load JARs on both driver and executors (also through submit args)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f'--jars \"{all_jars}\" pyspark-shell'\n",
    "\n",
    "# Initialize Spark with findspark\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Create Spark Session ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[1]\")  # More stable on Windows\n",
    "    .appName(\"SparkCassandraApp\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")  # Docker Desktop → Cassandra\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    .config(\"spark.python.worker.reuse\", \"false\")\n",
    "    .config(\"spark.driver.extraClassPath\", all_jars)\n",
    "    .config(\"spark.executor.extraClassPath\", all_jars)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"✅ Spark initialized — version:\", spark.version)\n",
    "print(\"✅ Spark JARs:\", spark.sparkContext.getConf().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21a606-736a-478d-92fc-10269da49279",
   "metadata": {},
   "source": [
    "print(\"JARs used :\", spark.sparkContext._conf.get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52c4c29-899c-41a9-b4c9-6dc333199e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostbyname(\"localhost\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e02a978-2570-434e-bdb9-8ea80c9eab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra is on 9042\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "s = socket.socket()\n",
    "s.settimeout(2)\n",
    "try:\n",
    "    s.connect((\"localhost\", 9042))\n",
    "    print(\"Cassandra is on 9042\")\n",
    "except Exception as e:\n",
    "    print(\" Cassandra not avaible\", e)\n",
    "s.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa830a-ebf1-4aa0-a024-453ba1234eb8",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Hadoop\\hadoop-3.3.1\"\n",
    "os.environ[\"PATH\"] = os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d22f9-a15a-413a-bb35-b9c332938e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b41d6ad3-1762-4ada-85e2-7cc680877c1f",
   "metadata": {},
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PYTHON_EXE = r\"C:\\Users\\muamb\\anaconda3\\envs\\IND320_env\\python.exe\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYTHON_EXE\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYTHON_EXE\n",
    "for bad in (\"PYTHONHOME\", \"PYTHONPATH\"):\n",
    "    os.environ.pop(bad, None)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_TOOL_OPTIONS\"] = \"-Djava.net.preferIPv4Stack=true\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"worker-smoke-test\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"✅ Spark:\", spark.version)\n",
    "print(\"✅ Python:\", sys.executable)\n",
    "print(\"➡️ Test workers =\", spark.sparkContext.parallelize(range(10)).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895a0e2-61b9-4cba-a187-c49e85cc3895",
   "metadata": {},
   "source": [
    "### **Main Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995ab11-103b-4657-ae8e-8363187a08a4",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "Cell In[17], line 20\n",
    "     18 print(\"✅ Spark:\", spark.version)\n",
    "     19 print(\"✅ Python:\", sys.executable)\n",
    "---> 20 print(\"➡️ Test workers =\", spark.sparkContext.parallelize(range(10)).count())\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2316, in RDD.count(self)\n",
    "   2295 def count(self) -> int:\n",
    "   2296     \"\"\"\n",
    "   2297     Return the number of elements in this RDD.\n",
    "   2298 \n",
    "   (...)   2314     3\n",
    "   2315     \"\"\"\n",
    "-> 2316     return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2291, in RDD.sum(self)\n",
    "   2270 def sum(self: \"RDD[NumberOrArray]\") -> \"NumberOrArray\":\n",
    "   2271     \"\"\"\n",
    "   2272     Add up the elements in this RDD.\n",
    "   2273 \n",
    "   (...)   2289     6.0\n",
    "   2290     \"\"\"\n",
    "-> 2291     return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n",
    "   2292         0, operator.add\n",
    "   2293     )\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:2044, in RDD.fold(self, zeroValue, op)\n",
    "   2039     yield acc\n",
    "   2041 # collecting result of mapPartitions here ensures that the copy of\n",
    "   2042 # zeroValue provided to each partition is unique from the one provided\n",
    "   2043 # to the final reduce call\n",
    "-> 2044 vals = self.mapPartitions(func).collect()\n",
    "   2045 return reduce(op, vals, zeroValue)\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\rdd.py:1833, in RDD.collect(self)\n",
    "   1831 with SCCallSiteSync(self.context):\n",
    "   1832     assert self.ctx._jvm is not None\n",
    "-> 1833     sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
    "   1834 return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322, in JavaMember.__call__(self, *args)\n",
    "   1316 command = proto.CALL_COMMAND_NAME +\\\n",
    "   1317     self.command_header +\\\n",
    "   1318     args_command +\\\n",
    "   1319     proto.END_COMMAND_PART\n",
    "   1321 answer = self.gateway_client.send_command(command)\n",
    "-> 1322 return_value = get_return_value(\n",
    "   1323     answer, self.gateway_client, self.target_id, self.name)\n",
    "   1325 for temp_arg in temp_args:\n",
    "   1326     if hasattr(temp_arg, \"_detach\"):\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179, in capture_sql_exception.<locals>.deco(*a, **kw)\n",
    "    177 def deco(*a: Any, **kw: Any) -> Any:\n",
    "    178     try:\n",
    "--> 179         return f(*a, **kw)\n",
    "    180     except Py4JJavaError as e:\n",
    "    181         converted = convert_exception(e.java_exception)\n",
    "\n",
    "File C:\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n",
    "    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n",
    "    325 if answer[1] == REFERENCE_TYPE:\n",
    "--> 326     raise Py4JJavaError(\n",
    "    327         \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "    328         format(target_id, \".\", name), value)\n",
    "    329 else:\n",
    "    330     raise Py4JError(\n",
    "    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\n",
    "    332         format(target_id, \".\", name, value))\n",
    "\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 1.0 failed 1 times, most recent failure: Lost task 9.0 in stage 1.0 (TID 21) (kubernetes.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
    "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
    "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
    "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
    "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
    "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
    "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
    "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
    "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
    "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
    "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
    "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
    "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
    "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
    "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
    "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
    "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
    "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
    "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
    "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
    "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
    "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
    "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
    "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
    "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
    "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
    "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
    "Caused by: java.io.EOFException\n",
    "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)\n",
    "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6009ecd0-c05f-40c2-81d2-c50572c8603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\muamb\\anaconda3\\envs\\IND320_py311\\python.exe\n",
      "Version: 3.11.14\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Version:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df852b08-83a4-48d7-9d53-68befc976bc1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9e5f9-37dc-4c94-8f14-0a9591983819",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eca195-6c9a-4370-b4e0-79e4fe269bca",
   "metadata": {},
   "source": [
    "## Part 2 : Work without Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57423b3a-041e-4d6a-83f0-2fd60098783c",
   "metadata": {},
   "source": [
    "### Install all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6e5280-1ed4-4ae8-80d3-6c05ca7e85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: cassandra-driver in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (3.29.3)\n",
      "Requirement already satisfied: pymongo in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (4.15.3)\n",
      "Requirement already satisfied: requests in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: plotly in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: geomet>=1.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from cassandra-driver) (1.1.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from pymongo) (2.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from plotly) (2.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from geomet>=1.1->cassandra-driver) (8.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muamb\\anaconda3\\envs\\ind320_py311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas cassandra-driver pymongo requests matplotlib plotly tqdm python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79bcb1-6e25-42b7-8742-360e0c4484b1",
   "metadata": {},
   "source": [
    "### Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61bc199-9fc0-4e93-91ad-f21a2b82a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79be934-aacf-4799-8f2f-2912c85c55d0",
   "metadata": {},
   "source": [
    "### Test of connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a90ff18-bee5-4357-a20c-f00b2105f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider  # only if your Cassandra requires authentication\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "# --- Connection settings ---\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "\n",
    "\n",
    "CASSANDRA_USER = None  \n",
    "CASSANDRA_PASS = None   \n",
    "\n",
    "KEYSPACE = \"ind320\"\n",
    "TABLE    = \"elhub_data_test\"  # just a temporary test table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c87ef67-cefe-4cdc-ab25-e3e5a56d6ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Cassandra\n"
     ]
    }
   ],
   "source": [
    "# --- Connect to Cassandra cluster ---\n",
    "try:\n",
    "    if CASSANDRA_USER and CASSANDRA_PASS:\n",
    "        auth_provider = PlainTextAuthProvider(\n",
    "            username=CASSANDRA_USER, password=CASSANDRA_PASS\n",
    "        )\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT, auth_provider=auth_provider)\n",
    "    else:\n",
    "        cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "\n",
    "    session = cluster.connect()\n",
    "    print(\"✅ Connected to Cassandra\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"❌ Unable to connect to Cassandra: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc8d91dc-7aad-4dce-8bae-ff170400821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keyspace `ind320` and table `elhub_data_test` are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Create keyspace if it does not exist ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "WITH replication = {{'class': 'SimpleStrategy', 'replication_factor': 1}};\n",
    "\"\"\")\n",
    "\n",
    "# --- Switch to the new keyspace ---\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# --- Create a simple test table ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    id int PRIMARY KEY,\n",
    "    txt text\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✅ Keyspace `{KEYSPACE}` and table `{TABLE}` are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572db58a-5a1b-49ca-96b9-eb1f126064fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Rows in the test table (sorted client-side):\n",
      "Row(id=1, txt='hello')\n",
      "Row(id=2, txt='world')\n",
      "Row(id=3, txt='ind320')\n"
     ]
    }
   ],
   "source": [
    "# Sanity check \n",
    "# --- Read without ORDER BY and sort in Python ---\n",
    "rows = session.execute(f\"SELECT id, txt FROM {TABLE};\")\n",
    "rows_sorted = sorted(rows, key=lambda r: r.id)  # sort locally in Python\n",
    "\n",
    "print(\"📄 Rows in the test table (sorted client-side):\")\n",
    "for r in rows_sorted:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc845da5-db75-4c50-81d0-5a5ba44b1086",
   "metadata": {},
   "source": [
    "## Load & clean (CSV → DataFrame pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3c88b-12da-4d4c-aefc-e71489073991",
   "metadata": {},
   "source": [
    "### Load the CSV, inspect it, keep only the useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad10163b-6469-4e70-b3c3-4724cbfacff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded CSV with shape: (661344, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>START_TIME</th>\n",
       "      <th>PRICE_AREA</th>\n",
       "      <th>PRODUCTION_GROUP</th>\n",
       "      <th>QUANTITY_KWH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-24T00:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1518343.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-24T01:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1508836.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-24T02:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1495758.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-24T03:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1491274.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-24T04:00:00.000+02:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1496936.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      START_TIME PRICE_AREA PRODUCTION_GROUP  QUANTITY_KWH\n",
       "0  2022-10-24T00:00:00.000+02:00        NO1            hydro   1518343.114\n",
       "1  2022-10-24T01:00:00.000+02:00        NO1            hydro   1508836.767\n",
       "2  2022-10-24T02:00:00.000+02:00        NO1            hydro   1495758.356\n",
       "3  2022-10-24T03:00:00.000+02:00        NO1            hydro   1491274.714\n",
       "4  2022-10-24T04:00:00.000+02:00        NO1            hydro   1496936.723"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- File path (adapt if you move the CSV) ---\n",
    "CSV_PATH = Path(r\"C:\\Users\\muamb\\Desktop\\ESILV\\2025-2026\\NMBU\\Cours\\IND320_DataToDecision\\IND320_DataTo_Decision\\data\\elhub_data.csv\")\n",
    "\n",
    "# --- Columns we care about for the assignment ---\n",
    "USECOLS = [\"START_TIME\", \"PRICE_AREA\", \"PRODUCTION_GROUP\", \"QUANTITY_KWH\"]\n",
    "\n",
    "# --- Read CSV efficiently: only needed columns, keep strings initially ---\n",
    "df_raw = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    usecols=USECOLS,\n",
    "    dtype={\n",
    "        \"START_TIME\": \"string\",\n",
    "        \"PRICE_AREA\": \"string\",\n",
    "        \"PRODUCTION_GROUP\": \"string\",\n",
    "        \"QUANTITY_KWH\": \"float64\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ Loaded CSV with shape:\", df_raw.shape)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07567702-37f6-43bd-b079-e7b14d1e91a5",
   "metadata": {},
   "source": [
    "### Minimal cleaning + time parsing (handle DST → store in UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3644e81b-dd13-4e32-9575-4690ff282322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with missing critical fields.\n",
      "Removed 0 duplicated hourly rows on key ['price_area', 'production_group', 'start_time'].\n",
      "✅ Clean DataFrame shape: (661344, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>quantity_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-23 22:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1518343.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-23 23:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1508836.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-24 00:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1495758.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-24 01:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1491274.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-24 02:00:00+00:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1496936.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start_time price_area production_group  quantity_kwh\n",
       "0 2022-10-23 22:00:00+00:00        NO1            hydro   1518343.114\n",
       "1 2022-10-23 23:00:00+00:00        NO1            hydro   1508836.767\n",
       "2 2022-10-24 00:00:00+00:00        NO1            hydro   1495758.356\n",
       "3 2022-10-24 01:00:00+00:00        NO1            hydro   1491274.714\n",
       "4 2022-10-24 02:00:00+00:00        NO1            hydro   1496936.723"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Basic column rename to a consistent snake_case schema ---\n",
    "df = df_raw.rename(columns={\n",
    "    \"START_TIME\": \"start_time\",\n",
    "    \"PRICE_AREA\": \"price_area\",\n",
    "    \"PRODUCTION_GROUP\": \"production_group\",\n",
    "    \"QUANTITY_KWH\": \"quantity_kwh\"\n",
    "}).copy()\n",
    "\n",
    "# --- Strip whitespace just in case ---\n",
    "df[\"price_area\"] = df[\"price_area\"].str.strip()\n",
    "df[\"production_group\"] = df[\"production_group\"].str.strip()\n",
    "\n",
    "# --- Parse ISO-8601 timestamps (they contain timezone offsets) to UTC ---\n",
    "# Note: errors='coerce' will produce NaT for bad rows; we'll drop those later.\n",
    "df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# --- Basic integrity checks ---\n",
    "n_before = len(df)\n",
    "df = df.dropna(subset=[\"start_time\", \"price_area\", \"production_group\", \"quantity_kwh\"])\n",
    "n_after = len(df)\n",
    "print(f\"Dropped {n_before - n_after} rows with missing critical fields.\")\n",
    "\n",
    "# --- Ensure proper dtypes ---\n",
    "df[\"price_area\"] = df[\"price_area\"].astype(\"string\")\n",
    "df[\"production_group\"] = df[\"production_group\"].astype(\"string\")\n",
    "df[\"quantity_kwh\"] = df[\"quantity_kwh\"].astype(\"float64\")\n",
    "\n",
    "# --- Optional: remove obvious duplicates if any (same key/hour) ---\n",
    "dedup_cols = [\"price_area\", \"production_group\", \"start_time\"]\n",
    "n_before = len(df)\n",
    "df = df.drop_duplicates(subset=dedup_cols, keep=\"last\")\n",
    "print(f\"Removed {n_before - len(df)} duplicated hourly rows on key {dedup_cols}.\")\n",
    "\n",
    "print(\"✅ Clean DataFrame shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59f587b0-a4f4-427a-a369-920f6328f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price areas: ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
      "Production groups: ['*', 'hydro', 'other', 'solar', 'thermal', 'wind']\n",
      "Date range (UTC): 2022-10-23 22:00:00+00:00 → 2025-10-23 21:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>n_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>other</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>thermal</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>wind</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NO2</td>\n",
       "      <td>hydro</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NO2</td>\n",
       "      <td>other</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NO2</td>\n",
       "      <td>thermal</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NO2</td>\n",
       "      <td>solar</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NO2</td>\n",
       "      <td>wind</td>\n",
       "      <td>26304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price_area production_group  n_rows\n",
       "0         NO1            hydro   26304\n",
       "1         NO1            other   26304\n",
       "2         NO1            solar   26304\n",
       "3         NO1          thermal   26304\n",
       "4         NO1             wind   26304\n",
       "6         NO2            hydro   26304\n",
       "7         NO2            other   26304\n",
       "9         NO2          thermal   26304\n",
       "8         NO2            solar   26304\n",
       "10        NO2             wind   26304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Quick sanity stats for documentation/log ---\n",
    "print(\"Price areas:\", sorted(df[\"price_area\"].dropna().unique().tolist()))\n",
    "print(\"Production groups:\", sorted(df[\"production_group\"].dropna().unique().tolist()))\n",
    "print(\"Date range (UTC):\", df[\"start_time\"].min(), \"→\", df[\"start_time\"].max())\n",
    "\n",
    "# --- Should be hourly frequency per area/group in 2021; quick check of counts ---\n",
    "counts = df.groupby([\"price_area\", \"production_group\"]).size().reset_index(name=\"n_rows\")\n",
    "counts.sort_values(\"n_rows\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d8c3e-1447-489c-aa10-2b7f41550e3e",
   "metadata": {},
   "source": [
    "### Create Cassandra table (final schema) + insert batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f477860-2f27-4425-a4fb-d8e98c450604",
   "metadata": {},
   "source": [
    "#### Creating the final table (key: (price_area, production_group), clustering: start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f4ccf-b3a5-4fac-b108-a5fb7ea114e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create final table for Elhub hourly production ---\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "KEYSPACE = \"ind320\"\n",
    "TABLE = \"elhub_production_mba_hour\"\n",
    "\n",
    "cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# Note: clustering order on start_time for efficient time-range scans within a partition\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    price_area text,\n",
    "    production_group text,\n",
    "    start_time timestamp,\n",
    "    quantity_kwh double,\n",
    "    PRIMARY KEY ((price_area, production_group), start_time)\n",
    ") WITH CLUSTERING ORDER BY (start_time ASC);\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Table `{KEYSPACE}.{TABLE}` ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a387c56-26e3-459e-9455-85e1a18d4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional year filter: keep only a given year if present ---\n",
    "TARGET_YEAR = 2021  # change to 2022/2023/etc. or set to None for \"no filter\"\n",
    "\n",
    "df_to_load = df.copy()\n",
    "if TARGET_YEAR is not None:\n",
    "    mask = df_to_load[\"start_time\"].dt.year == TARGET_YEAR\n",
    "    if mask.any():\n",
    "        df_to_load = df_to_load[mask].copy()\n",
    "        print(f\"ℹ️ Using only rows for year {TARGET_YEAR}: {len(df_to_load)} rows.\")\n",
    "    else:\n",
    "        print(f\"⚠️ No rows found for year {TARGET_YEAR}. Proceeding with ALL rows: {len(df_to_load)} rows.\")\n",
    "else:\n",
    "    print(f\"ℹ️ No year filter. Proceeding with ALL rows: {len(df_to_load)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e285d-0256-4586-a77a-a002b94ccda2",
   "metadata": {},
   "source": [
    "#### Remarks : I don't have any rows for 2021, but i'm sure i downloaded the good dataset, i'll try to find a solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2a1d7-fdb8-4c09-ae8b-ab9d66f3125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fast concurrent insert using PreparedStatement + execute_concurrent_with_args ---\n",
    "from cassandra.query import PreparedStatement\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Prepare the insert statement once (faster and safer)\n",
    "insert_ps: PreparedStatement = session.prepare(f\"\"\"\n",
    "    INSERT INTO {TABLE} (price_area, production_group, start_time, quantity_kwh)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "# Convert DataFrame rows to tuples expected by the driver\n",
    "params = [\n",
    "    (\n",
    "        str(row[\"price_area\"]),\n",
    "        str(row[\"production_group\"]),\n",
    "        row[\"start_time\"].to_pydatetime(),   # ensure native datetime (UTC)\n",
    "        float(row[\"quantity_kwh\"])\n",
    "    )\n",
    "    for _, row in df_to_load.iterrows()\n",
    "]\n",
    "\n",
    "# Tune concurrency for your machine; start modestly on Windows\n",
    "CONCURRENCY = 64  # you can try 128/256 if it runs smoothly\n",
    "BATCH_SIZE = 20_000  # chunk to avoid huge single job; adjust as needed\n",
    "\n",
    "total = len(params)\n",
    "print(f\" Inserting {total} rows into `{KEYSPACE}.{TABLE}` (concurrency={CONCURRENCY})\")\n",
    "\n",
    "errors_total = 0\n",
    "for i in tqdm(range(0, total, BATCH_SIZE), desc=\"Batches\"):\n",
    "    chunk = params[i:i+BATCH_SIZE]\n",
    "    results = execute_concurrent_with_args(\n",
    "        session,\n",
    "        insert_ps,\n",
    "        chunk,\n",
    "        concurrency=CONCURRENCY,\n",
    "        raise_on_first_error=False\n",
    "    )\n",
    "    # Count failed writes in this chunk\n",
    "    chunk_errors = sum(0 if ok else 1 for ok, _ in results)\n",
    "    errors_total += chunk_errors\n",
    "\n",
    "print(f\" Insert complete. Errors: {errors_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919ab06-b9f5-4498-9ac0-d581b931024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick counts per area/group (rough sanity check) ---\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT price_area, production_group, count(*) as n\n",
    "    FROM {TABLE}\n",
    "    GROUP BY price_area, production_group\n",
    "    ALLOW FILTERING;\n",
    "\"\"\")\n",
    "# Note: ALLOW FILTERING is okay here for small ad-hoc checks; avoid in prod.\n",
    "\n",
    "print(\"Counts by (price_area, production_group):\")\n",
    "for r in rows:\n",
    "    print(r)\n",
    "\n",
    "# --- Sample read: pick one area/group and a small time window ---\n",
    "sample_area = \"NO1\"\n",
    "sample_group = \"hydro\"\n",
    "\n",
    "rows = session.execute(f\"\"\"\n",
    "    SELECT price_area, production_group, start_time, quantity_kwh\n",
    "    FROM {TABLE}\n",
    "    WHERE price_area = %s AND production_group = %s\n",
    "    LIMIT 5;\n",
    "\"\"\", (sample_area, sample_group))\n",
    "\n",
    "print(\"\\n Sample rows:\")\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3fcb6e-e002-4724-907e-9e8aa06b4d1b",
   "metadata": {},
   "source": [
    "### Read from Cassandra and plots (pie + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3525cb1-db57-4aa6-bc33-e98f4ce86fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "# --- Cassandra connection (reuse your settings if already connected) ---\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "KEYSPACE       = \"ind320\"\n",
    "TABLE          = \"elhub_production_mba_hour\"\n",
    "\n",
    "cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# --- Known production groups (observed in your dataset) ---\n",
    "# Note: the '*' group can be excluded for charts if needed.\n",
    "KNOWN_GROUPS = [\"hydro\", \"wind\", \"solar\", \"thermal\", \"other\", \"*\"]\n",
    "\n",
    "def fetch_area_data(session, table, price_area, groups=None):\n",
    "    \"\"\"\n",
    "    Fetch all rows for a given price area across its production groups.\n",
    "    Because of the table's primary key ((price_area, production_group), start_time),\n",
    "    we must query per (price_area, production_group).\n",
    "    \"\"\"\n",
    "    if groups is None:\n",
    "        groups = KNOWN_GROUPS\n",
    "\n",
    "    rows_all = []\n",
    "    for g in groups:\n",
    "        # Skip empty group names just in case\n",
    "        if not g:\n",
    "            continue\n",
    "        # Query all rows for this (price_area, production_group)\n",
    "        stmt = SimpleStatement(f\"\"\"\n",
    "            SELECT price_area, production_group, start_time, quantity_kwh\n",
    "            FROM {table}\n",
    "            WHERE price_area = %s AND production_group = %s\n",
    "        \"\"\")\n",
    "        rs = session.execute(stmt, (price_area, g))\n",
    "        rows_all.extend(rs)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_area = pd.DataFrame(rows_all, columns=[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"])\n",
    "    if not df_area.empty:\n",
    "        # Ensure dtypes\n",
    "        df_area[\"start_time\"] = pd.to_datetime(df_area[\"start_time\"], utc=True)\n",
    "        df_area[\"quantity_kwh\"] = pd.to_numeric(df_area[\"quantity_kwh\"], errors=\"coerce\")\n",
    "    return df_area\n",
    "\n",
    "print(\"Helper ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a091041-1e85-4517-8776-81eb53c3473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# --- Choose a price area for the pie chart ---\n",
    "PRICE_AREA_FOR_PIE = \"NO1\"\n",
    "\n",
    "# --- Fetch data for that area ---\n",
    "df_area = fetch_area_data(session, TABLE, PRICE_AREA_FOR_PIE, groups=KNOWN_GROUPS)\n",
    "\n",
    "# --- Exclude the '*' group if you want cleaner charts ---\n",
    "df_pie = df_area[df_area[\"production_group\"] != \"*\"].copy()\n",
    "\n",
    "# --- Aggregate total kWh per production group ---\n",
    "totals = (\n",
    "    df_pie.groupby(\"production_group\", as_index=False)[\"quantity_kwh\"]\n",
    "    .sum()\n",
    "    .sort_values(\"quantity_kwh\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- Plot interactive pie chart with Plotly ---\n",
    "fig_pie = px.pie(\n",
    "    totals,\n",
    "    values=\"quantity_kwh\",\n",
    "    names=\"production_group\",\n",
    "    title=f\"Total production by group – {PRICE_AREA_FOR_PIE}\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3,\n",
    "    hole=0.3  # makes a donut chart\n",
    ")\n",
    "fig_pie.update_traces(textinfo=\"percent+label\", pull=[0.05]*len(totals))\n",
    "fig_pie.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bb100-67df-4a33-8a90-bb2f858a066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Choose a year/month for the line chart ---\n",
    "TARGET_YEAR  = 2022\n",
    "TARGET_MONTH = 10\n",
    "PRICE_AREA_FOR_LINE = \"NO1\"\n",
    "\n",
    "# --- Fetch data (reuse previous if same area) ---\n",
    "if PRICE_AREA_FOR_LINE != PRICE_AREA_FOR_PIE:\n",
    "    df_area_line = fetch_area_data(session, TABLE, PRICE_AREA_FOR_LINE, groups=KNOWN_GROUPS)\n",
    "else:\n",
    "    df_area_line = df_area.copy()\n",
    "\n",
    "# --- Convert UTC to Europe/Oslo ---\n",
    "df_area_line[\"start_time_oslo\"] = df_area_line[\"start_time\"].dt.tz_convert(\"Europe/Oslo\")\n",
    "\n",
    "# --- Filter by chosen month ---\n",
    "mask = (df_area_line[\"start_time_oslo\"].dt.year == TARGET_YEAR) & \\\n",
    "       (df_area_line[\"start_time_oslo\"].dt.month == TARGET_MONTH)\n",
    "df_month = df_area_line.loc[mask].copy()\n",
    "\n",
    "# --- Pivot for multi-line plotting ---\n",
    "df_month_pivot = (\n",
    "    df_month.pivot_table(\n",
    "        index=\"start_time_oslo\",\n",
    "        columns=\"production_group\",\n",
    "        values=\"quantity_kwh\",\n",
    "        aggfunc=\"sum\"\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# --- Create interactive line chart ---\n",
    "fig_line = go.Figure()\n",
    "for col in df_month_pivot.columns:\n",
    "    if col == \"*\":\n",
    "        continue\n",
    "    fig_line.add_trace(go.Scatter(\n",
    "        x=df_month_pivot.index,\n",
    "        y=df_month_pivot[col],\n",
    "        mode=\"lines\",\n",
    "        name=col\n",
    "    ))\n",
    "\n",
    "fig_line.update_layout(\n",
    "    title=f\"Hourly production – {PRICE_AREA_FOR_LINE} – {TARGET_YEAR}-{TARGET_MONTH:02d} (Europe/Oslo time)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Quantity (kWh)\",\n",
    "    legend_title=\"Production Group\",\n",
    "    template=\"plotly_white\",\n",
    "    hovermode=\"x unified\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_line.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041e1fb-9334-4dba-9b57-a9ba0d07de57",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bd4bd-ba00-44dd-b39b-bbe0bcaaa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Send cleaned data from Pandas DataFrame to MongoDB Atlas (secure version) ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load MongoDB credentials from .env (for security)\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "DB_NAME = \"ind320\"\n",
    "COLL_NAME = \"production_mba_hour\"\n",
    "\n",
    "# --- Connect to MongoDB ---\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "coll = db[COLL_NAME]\n",
    "\n",
    "# --- Clean collection (optional) ---\n",
    "coll.delete_many({})  # comment this line if you want to keep previous data\n",
    "\n",
    "# --- Prepare data to insert ---\n",
    "# (You already have df with columns price_area, production_group, start_time, quantity_kwh)\n",
    "docs = df[[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]].copy()\n",
    "docs[\"start_time\"] = docs[\"start_time\"].apply(lambda x: x.to_pydatetime())\n",
    "records = docs.to_dict(\"records\")\n",
    "\n",
    "# --- Insert into MongoDB ---\n",
    "result = coll.insert_many(records)\n",
    "print(f\"✅ Inserted {len(result.inserted_ids)} documents into MongoDB collection '{COLL_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50024083-0f87-42db-be01-4da03e91377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks\n",
    "# Quick Atlas check: list DBs, collections, and count documents\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://muambamvelejules_db_user:fLWyNBIoeBZs0lh4@cluster0.y22tbbi.mongodb.net/?appName=Cluster0\"\n",
    "DB_NAME = \"ind320\"\n",
    "COLL_NAME = \"production_mba_hour\"\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "# 1) List databases to confirm visibility\n",
    "print(\"Databases on this cluster:\")\n",
    "print(client.list_database_names())\n",
    "\n",
    "# 2) List collections inside DB_NAME\n",
    "db = client[DB_NAME]\n",
    "print(f\"Collections in '{DB_NAME}':\", db.list_collection_names())\n",
    "\n",
    "# 3) Count documents in the target collection\n",
    "coll = db[COLL_NAME]\n",
    "print(\"Document count in collection:\", coll.count_documents({}))\n",
    "\n",
    "# 4) Peek a few docs\n",
    "sample = coll.find({}, {\"_id\": 0}).limit(3)\n",
    "print(\"Sample docs:\")\n",
    "for d in sample:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc633-8a26-4cf1-97ea-b3783c70f48f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c07d40-ab1c-4335-8627-eaf8495b3e67",
   "metadata": {},
   "source": [
    "### Project Log – IND320 Part 2: Cassandra & MongoDB Integration\n",
    "\n",
    "This second part of the IND320 project focused on connecting local and cloud databases and visualizing energy production data through Streamlit.  \n",
    "The work started by setting up Cassandra locally as described in the course materials. Initially, several dependency and compatibility issues appeared with the Cassandra Python driver and Python 3.12. After testing different environments, a stable setup was achieved using Python 3.11. The Cassandra connection was verified by creating a keyspace and a small test table, inserting a few rows, and reading them back.\n",
    "\n",
    "Next, the dataset “elhub_data.csv” was loaded using Pandas. This dataset contains hourly electricity production data by price area and production group. The relevant columns were extracted (`price_area`, `production_group`, `start_time`, `quantity_kwh`) and stored in Cassandra. A sanity check confirmed that each price area and production group combination contained the expected number of rows.\n",
    "\n",
    "As the teacher allowed using MongoDB instead of Spark for simplicity, the cleaned data was then inserted into a MongoDB Atlas cluster. The data transfer from Pandas to MongoDB was done using `pymongo`, with more than 600,000 documents successfully inserted. This ensured that all data was accessible from the cloud for further visualization.\n",
    "\n",
    "In the Streamlit app, a new page (“Production Analysis”) was created to access MongoDB directly. The app reads the data securely using a `secrets.toml` file to hide credentials. Two visualizations were implemented with Plotly:\n",
    "1. A **pie chart** showing the total annual production by group for a selected price area.\n",
    "2. A **line chart** showing hourly production for selected groups and months.\n",
    "\n",
    "The app structure now includes multiple pages and reusable helper functions in a `utils.py` file. The use of Plotly instead of Matplotlib improved the interactivity and aesthetics of the graphs, following best practices mentioned by the teacher.  \n",
    "The full workflow now runs end-to-end: local preprocessing and insertion in MongoDB, and cloud visualization via Streamlit.\n",
    "\n",
    "Overall, this part of the project improved my understanding of how to combine local and remote databases in Python and how to deploy data-driven web applications. It also reinforced good practices in version control by using separate Git branches (`main` and `part2`) to organize updates before merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba41d1d-5b0d-4467-8b95-54025ed387ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
